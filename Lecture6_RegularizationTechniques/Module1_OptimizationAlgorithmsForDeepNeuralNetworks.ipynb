{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module1_OptimizationAlgorithmsForDeepNeuralNetworks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rtxEC-Rwrvv"
      },
      "source": [
        "# Optimization Algorithms for Deep Neural Networks\n",
        "\n",
        "In this module, we will study:\n",
        "1. Gradient Descent\n",
        "2. Momentum-Based Gradient Descent\n",
        "3. Nesterov Mementum\n",
        "4. Adagrad\n",
        "5. RMSProp\n",
        "6. Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaGbgMHt16DS"
      },
      "source": [
        "## Optimization Algorithms\n",
        "\n",
        "**What is optimization?**\n",
        "\n",
        "In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function.\n",
        "\n",
        "In the case of Machine Learning or Deep Learning, optimization refers to _the process of minimizing the loss function by systematically updating the network weights_. \n",
        "\n",
        "Mathematically, this is expressed as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "w = argmin_wL(w),\n",
        "\\end{equation}\n",
        "\n",
        "where $L(w)$ and $w$ denotes, respectively, the loss function and weights.\n",
        "\n",
        "***\n",
        "\n",
        "**The Task of an Optimization Algorithm:**\n",
        "\n",
        "Optimization algorithms (in the case of minimization) have one of the following goals:\n",
        "1. Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum.\n",
        "2. Find the lowest possible value of the objective function within its neighborhood. That‚Äôs usually the case if the objective function is not convex as the case in most deep learning problems.\n",
        "\n",
        "There are several optimization techniques, we will, in this module, learn the important once.\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TURMOyXt3SbC"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POkIk3E07TWE"
      },
      "source": [
        "#### What is Gradient Descent?\n",
        "\n",
        "Gradient Descent is an optimizing algorithm used in Machine/ Deep Learning algorithms. It is a first-order (i.e., gradient-based) optimization algorithm where we iteratively update the parameters of a differentiable cost function until its minimum is attained.\n",
        "\n",
        "Mathematically, gardient descent can be defined as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "w := w - \\eta . \\frac{\\partial}{\\partial w} L(w)\n",
        "\\end{equation}\n",
        "\n",
        "In the above equation, $\\eta$ denotes the learning rate.\n",
        "\n",
        "***\n",
        "\n",
        "Visually, the process of gradient descent optimization can be shown as in the following figure.\n",
        "\n",
        "![GradientDescent](gradient-descent.png)\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRpe7gO27QlE"
      },
      "source": [
        "#### Steps to Implement Gradient Descent\n",
        "\n",
        "1. Randomly initialize values for weights.\n",
        "2. Update weights using the following formula.\n",
        "\\begin{equation}\n",
        "w := w - \\eta . \\frac{\\partial}{\\partial w} L(w)\n",
        "\\end{equation}\n",
        "\n",
        "3. Repeat until slope = 0; that is, $\\frac{\\partial}{\\partial w} L(w) = 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhO-T2Fc6nAu"
      },
      "source": [
        "***\n",
        "#### Selection of Learning Rate \n",
        "\n",
        "Learning rate must be chosen wisely as:\n",
        "\n",
        "1. if it is too small, then the model will take some time to learn.\n",
        "2. if it is too large, model will converge as our pointer will shoot and we‚Äôll not be able to get to minima as shown in the following figure.\n",
        "\n",
        "![LearningRate](image1.png)\n",
        "\n",
        "\n",
        "***\n",
        "![LearningRateSelection](image2.png)\n",
        "\n",
        "****\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q56Oatbc7Ve7"
      },
      "source": [
        "There are three variants of gradient descent: 1. Batch gradient descent, 2: stochastic gradient descent, and 3. mini-batch gradient descent.\n",
        "\n",
        "### Batch Gradient Descent\n",
        "\n",
        "In this variant, we calculate the gradient for the entire dataset on each training step before we update the weights.\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial w} L(w) = \\frac{1}{N} \\sum_{i=1}^{N}\\frac{\\partial}{\\partial w} L_i(x_i, y_i, w)\n",
        "\\end{equation}\n",
        "\n",
        "You can imagine that since we take the sum of the loss of all individual training examples, our computation becomes quickly very expensive. Therefore it‚Äôs impractical for large datasets.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUamshiN8gWH"
      },
      "source": [
        "### Stochastic gradient descent\n",
        "\n",
        "Stochastic Gradient Descent (SGD) was introduced to address this exact issue. Instead of calculating the gradient over all training examples and update the weights, SGD updates the weights for each training example $x_i,y_i$ \n",
        "\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "w := w - \\eta \\frac{\\partial}{\\partial w} L_i(x_i, y_i, w)\n",
        "\\end{equation}\n",
        "\n",
        "As a result, SGD is much faster and more computationally efficient, but it has noise in the estimation of the gradient. Since it updates the weight frequently, it can lead to big oscillations, which makes the training process highly unstable.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFr8CUWu9E3d"
      },
      "source": [
        "### Mini-batch Stochastic Gradient Descent\n",
        "Mini batch SGD sits right in the middle of the two previous ideas combining the best of both worlds. It randomly selects $n$ training examples, the so-called mini-batch, from the whole dataset and computes the gradients only from them. It essentially tries to approximate Batch Gradient Descent by sampling only a subset of the data. Mathematically:\n",
        "\n",
        "\\begin{equation}\n",
        "w := w - \\eta \\frac{\\partial}{\\partial w} L_i(x_{(i:i+n)}, y_{(i:i+n)}, w)\n",
        "\\end{equation}\n",
        "\n",
        "In practice, mini-batch SGD is the most frequently used variation because it‚Äôs both computationally cheap and results in more robust convergence.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btoA2tiQ9g_F"
      },
      "source": [
        "***\n",
        "### Concerns on SGD\n",
        "\n",
        "SGD is easy to implement, but it has some limitations:\n",
        "\n",
        "1. If the loss function changes quickly in one direction and slowly in another, it may result in a high oscillation of gradients making the training progress very slow.\n",
        "\n",
        "2. If the loss function has a local minimum or a saddle point, it is very possible that SGD will be stuck there without being able to ‚Äújump out‚Äù and proceed in finding a better minimum. This happens because the gradient becomes zero so there is no update in the weight whatsoever.\n",
        "\n",
        " **A saddle point is a point on the surface of the graph of a function where the slopes (derivatives) are all zero but which is not a local maximum of the function.**\n",
        "\n",
        "3. The gradients are still noisy because we estimate them based only on a small sample of our dataset. The noisy updates might not correlate well with the true direction of the loss function.\n",
        "\n",
        "4. Choosing a good loss function is tricky and requires time-consuming experimentation with different hyperparameters.\n",
        "\n",
        "5. The same learning rate is applied to all of our parameters, which can become problematic for features with different frequencies or significance.\n",
        "\n",
        "To overcome some of these problems, many improvements have been proposed over the years.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0HRfGco-SO1"
      },
      "source": [
        "***\n",
        "### [Momentum-Based SGD](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d) \n",
        "\n",
        "The overcome the limitation of SGD, a variation, called SGD with momentum, is proposed. For the best explanation, please click on the title of the cell.\n",
        "\n",
        "Mathematically, the SGD with momentum can be defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "V_t = \\beta V_{t-1} + (1-\\beta) \\frac{\\partial}{\\partial w}L(x, y, w) \\\\\n",
        "W := W - \\eta V_t\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "- $L$ is the loss function\n",
        "- $\\eta$ is the learning rate\n",
        "- $\\beta$ is the constant, called momentum, and its ideal value is $0.9$.\n",
        "- $V_t$ is a term, called velocity.\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7jQvvInkMxT"
      },
      "source": [
        "#### Advantages of Using SGD with Momentum\n",
        "\n",
        "1. We can now escape local minimums or saddle points because we keep moving downwards even though the gradient of the mini-batch might be zero\n",
        "\n",
        "2. Momentum can also help us reduce the oscillation of the gradients because the velocity vectors can smooth out these highly changing landscapes.\n",
        "\n",
        "3. Finally, it reduces the noise of the gradients (stochasticity) and follows a more direct walk down the landscape.\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLEAl2dCkgUi"
      },
      "source": [
        "### Nestrov Accelarated Gradient\n",
        "\n",
        "An alternative version of momentum, called Nesterov momentum, calculates the update direction in a slightly different way.\n",
        "\n",
        "Instead of combining the velocity vector and the gradients, we calculate where the velocity vector would take us and compute the gradient at this point. In other words, we find what the gradient vector would have been if we moved only according to our build-up velocity, and compute it from there.\n",
        "\n",
        "We can visualize this as below:\n",
        "\n",
        "![NAG](image3.png)\n",
        "\n",
        "This anticipatory update prevents us from going too fast and results in increased responsiveness. The most famous algorithm that make us of Nesterov momentum is called Nesterov accelerated gradient (NAG) and goes as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "V_t = \\beta V_{t-1} + (1-\\beta) \\frac{\\partial}{\\partial w}L(X, y, w+\\beta V_{t-1}) \\\\\n",
        "W := W - \\eta V_t\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "- $L$ is the loss function\n",
        "- $\\eta$ is the learning rate\n",
        "- $\\beta$ is the constant, called momentum, and its ideal value is $0.9$.\n",
        "- $V_t$ is a term, called velocity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf_2FXDal3DE"
      },
      "source": [
        "***\n",
        "### SGD in Keras\n",
        "\n",
        "**In Keras, we can use following code to implement SGD.**\n",
        "```\n",
        "tf.keras.optimizers.SGD(\n",
        "    learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\", **kwargs\n",
        ")\n",
        "```\n",
        "**1. Keeping momentum zero and nesterove false, the above code will implement simple SGD.**\n",
        "\n",
        "**2. Keeping momentum any value between 0 and 1, and nesterove false, will work as SGD with momentum.**\n",
        "\n",
        "**3. Keeping momentum a value and nestrove true will work as SGD with momentum and nestrove.**\n",
        "\n",
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuCLA5J2jtD7"
      },
      "source": [
        "## Adaptive Learning Rate\n",
        "\n",
        "The other big idea of optimization algorithms is adaptive learning rate. The intuition is that we‚Äôd like to perform smaller updates for frequent features and bigger ones for infrequent ones. This will allow us to overcome some of the problems of SGD mentioned before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXMLI1rKjtD8"
      },
      "source": [
        "### Adagrad\n",
        "\n",
        "A limitation with all the optimization techniques we have learnt so far is that the hyperparameter called learning rate is fixed. \n",
        "\n",
        "Unfortunately, this hyper-parameter could be very difficult to set because if we set it too small, then the parameter update will be very slow and it will take very long time to achieve an acceptable loss. Otherwise, if we set it too large, then the parameter will move all over the function and may never achieve acceptable loss at all. To make things worse, the high-dimensional non-convex nature of neural networks optimization could lead to different sensitivity on each dimension. The learning rate could be too small in some dimension and could be too large in another dimension.\n",
        "\n",
        "One obvious way to mitigate that problem is to choose different learning rate for each dimension, but imagine if we have thousands or millions of dimensions, which is normal for deep neural networks, that would not be practical. So, in practice, one of the earlier algorithms that have been used to mitigate this problem for deep neural networks is the [AdaGrad algorithm]((https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)). This algorithm adaptively scaled the learning rate for each dimension.\n",
        "\n",
        "Mathematically, AdaGrad can be represented as follows:\n",
        "\\begin{equation}\n",
        "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\epsilon I + diag(G_t)}}.g_t\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "- $t$ is the time step; for instance, $w_t$ is the current weight value and $w_{t+1}$ is the next weight value\n",
        "- $w$ are the weight parameters to be optimized\n",
        "- $\\eta$ is the initial learning rate\n",
        "- $\\epsilon$ is some small quantity that is used to avoid the division by zero\n",
        "- $I$ is the identity matrix\n",
        "- $g_t$ is the gradient estimate, and it is calculated as follows:\n",
        "\\begin{equation}\n",
        "g_t = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\partial}{\\partial w}L\\left(x^i, y^i, w\\right)\n",
        "\\end{equation}\n",
        "- The key of this algorithm is in the matrix $G_t$ \n",
        "which is the sum of the outer product of the gradients until time-step ùë°, which is defined by\n",
        "\\begin{equation}\n",
        "G_t = \\sum_{\\tau=1}^{t}g_{\\tau} g_{\\tau}^T, \\ \\text{T reprsents transpose operation.}\n",
        "\\end{equation}\n",
        "\n",
        "**Actually, we can use the full matrix Gùë° in the parameter update, but computing the square root of the full matrix is impractical, especially in very high dimension. On the other hand, computing the square root and the inverse of only the diagonal diag(Gùë°) can easily be done.** Therefore, the above equation will reduce to\n",
        "\n",
        "\\begin{equation}\n",
        "diag(G_t) = \n",
        "\\begin{bmatrix}\n",
        "G_t^{(1,1)} & 0 & \\dots & 0 \\\\\n",
        "0 & G_t^{(2,2)} & \\dots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\dots & G_t^{(m,m)} \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exta75fKo4Rk"
      },
      "source": [
        "#### AdaGrad Explanation\n",
        "\n",
        "The AdaGrad equation given below:\n",
        "\\begin{equation}\n",
        "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\epsilon I + diag(G_t)}}.g_t\n",
        "\\end{equation}\n",
        "\n",
        "reduces to\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "w_{t+1}^{(1)} \\\\\n",
        "w_{t+1}^{(2)}  \\\\\n",
        "\\vdots  \\\\\n",
        "w_{t+1}^{(m)} \\\\\n",
        "\\end{bmatrix} = \n",
        "\\begin{bmatrix}\n",
        "w_t^{(1)} \\\\\n",
        "w_t^{(2)}  \\\\\n",
        "\\vdots  \\\\\n",
        "w_t^{(m)} \\\\\n",
        "\\end{bmatrix} - \\eta \\left(\n",
        "  \\begin{bmatrix}\n",
        "\\epsilon & 0 & \\dots & 0 \\\\\n",
        "0 & \\epsilon & \\dots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\dots & \\epsilon \\\\\n",
        "\\end{bmatrix} +\n",
        "\\begin{bmatrix}\n",
        "G_t^{(1,1)} & 0 & \\dots & 0 \\\\\n",
        "0 & G_t^{(2,2)} & \\dots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\dots & G_t^{(m,m)} \\\\\n",
        "\\end{bmatrix}\n",
        "  \\right)^{\\frac{-1}{2}}.\\begin{bmatrix}\n",
        "g_t^{(1)} \\\\\n",
        "g_t^{(2)}  \\\\\n",
        "\\vdots  \\\\\n",
        "g_t^{(m)} \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "After simplification, the above equation will reduce to\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "w_{t+1}^{(1)} \\\\\n",
        "w_{t+1}^{(2)}  \\\\\n",
        "\\vdots  \\\\\n",
        "w_{t+1}^{(m)} \\\\\n",
        "\\end{bmatrix} = \n",
        "\\begin{bmatrix}\n",
        "w_t^{(1)} \\\\\n",
        "w_t^{(2)}  \\\\\n",
        "\\vdots  \\\\\n",
        "w_t^{(m)} \\\\\n",
        "\\end{bmatrix} - \n",
        "  \\begin{bmatrix}\n",
        "\\frac{\\eta}{\\sqrt{\\epsilon+G_t^{(1,1)}}} & 0 & \\dots & 0 \\\\\n",
        "0 & \\frac{\\eta}{\\sqrt{\\epsilon+G_t^{(2,2)}}} & \\dots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\dots & \\frac{\\eta}{\\sqrt{\\epsilon+G_t^{(m,m}}} \\\\\n",
        "\\end{bmatrix}\n",
        "  . \\begin{bmatrix}\n",
        "g_t^{(1)} \\\\\n",
        "g_t^{(2)}  \\\\\n",
        "\\vdots  \\\\\n",
        "g_t^{(m)} \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "After multiplying the effective learning rate matrix with the gradient estimate vector yields the update rule of AdaGrad\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "w_{t+1}^{(1)} \\\\\n",
        "w_{t+1}^{(2)}  \\\\\n",
        "\\vdots  \\\\\n",
        "w_{t+1}^{(m)} \\\\\n",
        "\\end{bmatrix} = \n",
        "\\begin{bmatrix}\n",
        "w_t^{(1)} \\\\\n",
        "w_t^{(2)}  \\\\\n",
        "\\vdots  \\\\\n",
        "w_t^{(m)} \\\\\n",
        "\\end{bmatrix} - \n",
        "  \\begin{bmatrix}\n",
        "\\frac{\\eta}{\\sqrt{\\epsilon+G_t^{(1,1)}}}g_t^{(1)}  \\\\\n",
        " \\frac{\\eta}{\\sqrt{\\epsilon+G_t^{(2,2)}}}g_t^{(1)}  \\\\\n",
        "\\vdots \\\\\n",
        " \\frac{\\eta}{\\sqrt{\\epsilon+G_t^{(m,m)}}}g_t^{(1)} \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "If we compare this with previously discussed algorithms, for example Stochastic Gradient Descent, which update in this form is\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "w_{t+1}^{(1)} \\\\\n",
        "w_{t+1}^{(2)}  \\\\\n",
        "\\vdots  \\\\\n",
        "w_{t+1}^{(m)} \\\\\n",
        "\\end{bmatrix} = \n",
        "\\begin{bmatrix}\n",
        "w_t^{(1)} \\\\\n",
        "w_t^{(2)}  \\\\\n",
        "\\vdots  \\\\\n",
        "w_t^{(m)} \\\\\n",
        "\\end{bmatrix} - \n",
        "  \\begin{bmatrix}\n",
        "\\eta g_t^{(1)}  \\\\\n",
        "\\eta g_t^{(1)}  \\\\\n",
        "\\vdots \\\\\n",
        "\\eta g_t^{(1)} \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "**we can see that Stochastic Gradient Decent use same learning rate at each iteration in all dimension. On the other hand, AdaGrad adaptively scaled the learning rate with respect to the accumulated squared gradient at each iteration in each dimension.**\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-lTXyBDyjO6"
      },
      "source": [
        "#### AdaGrad in Keras\n",
        "\n",
        "In Keras, we can use following code to implement AdaGrad.\n",
        "```\n",
        "tf.keras.optimizers.Adagrad(\n",
        "    learning_rate=0.001,\n",
        "    initial_accumulator_value=0.1,\n",
        "    epsilon=1e-07,\n",
        "    name=\"Adagrad\",\n",
        "    **kwargs\n",
        ")\n",
        "```\n",
        "\n",
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s40mRyoC3IEq"
      },
      "source": [
        "### RMSProp\n",
        "\n",
        "A big drawback of Adagrad is that as time goes by, the learning rate becomes smaller and smaller due to the monotonic increment of the running squared sum.\n",
        "\n",
        "RMSprop is a gradient based optimization technique used in training neural networks. It was proposed by the father of back-propagation, Geoffrey Hinton. Gradients of very complex functions like neural networks have a tendency to either vanish or explode as the data propagates through the function (*refer to vanishing gradients problem). Rmsprop was developed as a stochastic technique for mini-batch learning.\n",
        "\n",
        "\n",
        "RMSprop deals with the above issue by using a moving average of squared gradients to normalize the gradient. This normalization balances the step size‚Ää (momentum), ‚Äädecreasing the step for large gradients to avoid exploding, and increasing the step for small gradients to avoid vanishing. \n",
        "\n",
        "Simply put, RMSprop uses an adaptive learning rate instead of treating the learning rate as a hyperparameter. This means that the learning rate changes over time.\n",
        "\n",
        "Mathematically, RMSprop can be represented as follows:\n",
        "\\begin{equation}\n",
        "w := w - \\eta . \\frac{1}{\\sqrt{v_{dw}}+\\epsilon}\\frac{\\partial}{\\partial w}L\\left(X, y, w\\right) \n",
        "\\end{equation}\n",
        "In the above equation:\n",
        "1. $w$ are the parameters to be updated, and in the given case, they are weights and biases.\n",
        "2. $\\epsilon$ is a very very small term to avoid any numerical instability, which is usually $1e^{-7}$.\n",
        "3. $v_{dw}$ is the moving average of the loss function, and it is calculated as follows:\n",
        "\\begin{equation}\n",
        "v_{dw} = \\beta v_{dw} + (1-\\beta) \\left(\\frac{\\partial}{\\partial w}L\\left(X, y, w\\right) \\right)^2\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l8clayt7V8F"
      },
      "source": [
        "#### RMSProp in Keras\n",
        "\n",
        "In Keras, we can use following code to implement RMSProp.\n",
        "```\n",
        "tf.keras.optimizers.RMSprop(\n",
        "    learning_rate=0.001,\n",
        "    rho=0.9,\n",
        "    momentum=0.0,\n",
        "    epsilon=1e-07,\n",
        "    centered=False,\n",
        "    name=\"RMSprop\",\n",
        "    **kwargs\n",
        ")\n",
        "```\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf0KBxVU8JfV"
      },
      "source": [
        "### Adam\n",
        "\n",
        "Adam (Adaptive moment estimation) can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum. \n",
        "\n",
        "The Adam optimization works as shown in the following equation.\n",
        "\n",
        "\\begin{equation}\n",
        "w := w - \\eta \\frac{\\hat{m}_{dw}}{\\sqrt{\\hat{v}_{dw}}+\\epsilon}\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "1. $\\eta$ is the learning rate\n",
        "2. $\\epsilon$ is used to avoid the division by zero\n",
        "3. $\\hat{m}_{dw}$ is the first moment and called momentum, and it is computed as follows:\n",
        "\\begin{equation}\n",
        "\\hat{m}_{dw} = \\frac{m_{dw}}{1-\\beta_1^t}\n",
        "\\end{equation}\n",
        "4. $\\hat{v}_{dw}$ is the second moment and called the velocity, and it is computed as follows:\n",
        "\\begin{equation}\n",
        "\\hat{v}_{dw} = \\frac{v_{dw}}{1-\\beta_2^t}\n",
        "\\end{equation}\n",
        "\n",
        "Two terms: $m_{dw}$ and $v_{dw}$, are the stochastic gardient descent with momentum and RMSProp, respectively, and are calculated as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "m_{dw} = \\beta_1 m_{dw} + (1-\\beta_2)\\frac{\\partial}{\\partial w}L(X, y, w)\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "v_{dw} = \\beta_1 v_{dw} + (1-\\beta_2)\\left(\\frac{\\partial}{\\partial w}L(X, y, w)\\right)^2\n",
        "\\end{equation}\n",
        "\n",
        "The parameters: $\\beta_1$ and $\\beta_2$ are exponential decaly rate for the first moment ($m_{dw}$), and exponential decay rate for the second moment ($v_{dw}$), respectively. Usually, $\\beta_1 = 0.9$ and $\\beta_2=0.999$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PplcY-cyFWsa"
      },
      "source": [
        "***\n",
        "#### Adam in Keras\n",
        "\n",
        "In Keras, we can use following code to implement Adam.\n",
        "\n",
        "```\n",
        "tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    name=\"Adam\",\n",
        "    **kwargs\n",
        ")\n",
        "```\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqktFEiGFhmT"
      },
      "source": [
        "***\n",
        "**Please note that several optimization techniques are available in the literature; however, we have discussed only the most frequently used ones. Nonetheless, you are advised to explore other optimization techniques as well.**\n",
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSkbXqI1F476"
      },
      "source": [
        "## Visuallizing Optimizers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkHy_znQHavU"
      },
      "source": [
        "Algorithms with momentum have a smoother trajectory than non-momentum based but this may result in overshooting.\n",
        "\n",
        "![Gify1](giphy1.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYXdfxfOVJ4a"
      },
      "source": [
        "Methods with an adaptive learning rate have a faster convergence rate, better stability, and less jittering.\n",
        "\n",
        "![Gify1](giphy2.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkNKknmGVQLK"
      },
      "source": [
        "Algorithms that do not scale the step size (adaptive learning rate) have a harder time to escape local minimums and break the symmetry of the loss function\n",
        "\n",
        "\n",
        "![Gify1](giphy3.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqqaF53HVY4i"
      },
      "source": [
        "Saddle points cause momentum-based methods to oscillate before finding the correct downhill path.\n",
        "\n",
        "![Gify1](giphy4.gif)\n",
        "\n",
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwW3C2TH0p9h"
      },
      "source": [
        "## References\n",
        "\n",
        "1. [A journey into Optimization algorithms for Deep Neural Networks](https://theaisummer.com/optimization/)\n",
        "2. [The Hitchhiker‚Äôs Guide to Optimization in Machine Learning](https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-machine-learning-edcf5a104210)\n",
        "3. [Gradient Descent Explained](https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c)\n",
        "4. [Gentle Introduction to the Adam Optimization Algorithm for Deep Learning](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
        "5. [Stochastic Gradient Descent with momentum](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d)\n",
        "6. [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)\n",
        "7. [Gradient Descent With AdaGrad From Scratch](https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/)\n",
        "8. [An Introduction to AdaGrad](https://medium.com/konvergen/an-introduction-to-adagrad-f130ae871827)\n",
        "9. [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
        "10. [What is RMSprop?](https://deepai.org/machine-learning-glossary-and-terms/rmsprop)\n",
        "11. [Understanding RMSprop](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a)\n",
        "12. [A Look at Gradient Descent and RMSprop Optimizers](https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b)\n",
        "13. [How to implement an Adam Optimizer from Scratch](https://towardsdatascience.com/how-to-implement-an-adam-optimizer-from-scratch-76e7b217f1cc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5n4RTu3wmfI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}