{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Module1_OptimizationAlgorithmsForDeepNeuralNetworks.ipynb","provenance":[{"file_id":"1-FvUj5f6x00GJncw238nLzAF2Cckq9Hn","timestamp":1626162014105}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNSuMWsy/w10QiIXE3lGFg4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3rtxEC-Rwrvv"},"source":["# Optimization Algorithms for Deep Neural Networks\n","\n","In this module, we will study:\n","1. Gradient Descent\n","2. Momentum-Based Gradient Descent\n","3. Nesterov Mementum\n","4. Adagrad\n","5. RMSProp\n","6. Adam\n"]},{"cell_type":"markdown","metadata":{"id":"zaGbgMHt16DS"},"source":["## Optimization Algorithms\n","\n","**What is optimization?**\n","\n","In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function.\n","\n","In the case of Machine Learning or Deep Learning, optimization refers to _the process of minimizing the loss function by systematically updating the network weights_. \n","\n","Mathematically, this is expressed as follows:\n","\n","\\begin{equation}\n","w = argmin_wL(w),\n","\\end{equation}\n","\n","where $L(w)$ and $w$ denotes, respectively, the loss function and weights.\n","\n","***\n","\n","**The Task of an Optimization Algorithm:**\n","\n","Optimization algorithms (in the case of minimization) have one of the following goals:\n","1. Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum.\n","2. Find the lowest possible value of the objective function within its neighborhood. That’s usually the case if the objective function is not convex as the case in most deep learning problems.\n","\n","There are several optimization techniques, we will, in this module, learn the important once.\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"TURMOyXt3SbC"},"source":["## Gradient Descent\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"POkIk3E07TWE"},"source":["#### What is Gradient Descent?\n","\n","Gradient Descent is an optimizing algorithm used in Machine/ Deep Learning algorithms. It is a first-order (i.e., gradient-based) optimization algorithm where we iteratively update the parameters of a differentiable cost function until its minimum is attained.\n","\n","Mathematically, gardient descent can be defined as follows:\n","\n","\\begin{equation}\n","w := w - \\eta . \\frac{\\partial}{\\partial w} L(w)\n","\\end{equation}\n","\n","In the above equation, $\\eta$ denotes the learning rate.\n","\n","***\n","\n","Visually, the process of gradient descent optimization can be shown as in the following figure.\n","\n","![GradientDescent](gradient-descent.png)\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"XRpe7gO27QlE"},"source":["#### Steps to Implement Gradient Descent\n","\n","1. Randomly initialize values for weights.\n","2. Update weights using the following formula.\n","\\begin{equation}\n","w := w - \\eta . \\frac{\\partial}{\\partial w} L(w)\n","\\end{equation}\n","\n","3. Repeat until slope = 0; that is, $\\frac{\\partial}{\\partial w} L(w) = 0$."]},{"cell_type":"markdown","metadata":{"id":"yhO-T2Fc6nAu"},"source":["***\n","#### Selection of Learning Rate \n","\n","Learning rate must be chosen wisely as:\n","\n","1. if it is too small, then the model will take some time to learn.\n","2. if it is too large, model will converge as our pointer will shoot and we’ll not be able to get to minima as shown in the following figure.\n","\n","![LearningRate](image1.png)\n","\n","\n","***\n","![LearningRateSelection](image2.png)\n","\n","****\n"]},{"cell_type":"markdown","metadata":{"id":"q56Oatbc7Ve7"},"source":["There are three variants of gradient descent: 1. Batch gradient descent, 2: stochastic gradient descent, and 3. mini-batch gradient descent.\n","\n","Batch gradient descent\n","The equation and code presented above actually referred to batch gradient descent. In this variant, we calculate the gradient for the entire dataset on each training step before we update the weights.\n","\n","\\begin{equation}\n","w := w - \\eta . \\frac{\\partial}{\\partial w} L(w)\n","\\end{equation}\n","You can imagine that since we take the sum of the loss of all individual training examples, our computation becomes quickly very expensive. Therefore it’s impractical for large datasets."]},{"cell_type":"markdown","metadata":{"id":"LwW3C2TH0p9h"},"source":["## References\n","\n","1. [A journey into Optimization algorithms for Deep Neural Networks](https://theaisummer.com/optimization/)\n","2. [The Hitchhiker’s Guide to Optimization in Machine Learning](https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-machine-learning-edcf5a104210)\n","3. [Gradient Descent Explained](https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c)\n","4. [Gentle Introduction to the Adam Optimization Algorithm for Deep Learning](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)"]},{"cell_type":"code","metadata":{"id":"b5n4RTu3wmfI"},"source":[""],"execution_count":null,"outputs":[]}]}