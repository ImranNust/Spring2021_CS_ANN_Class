{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Module1_OptimizationAlgorithmsForDeepNeuralNetworks.ipynb","provenance":[{"file_id":"1-FvUj5f6x00GJncw238nLzAF2Cckq9Hn","timestamp":1626162014105}],"collapsed_sections":[],"authorship_tag":"ABX9TyMdOpfQsJvHpwd+8A3x1QmY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3rtxEC-Rwrvv"},"source":["# Optimization Algorithms for Deep Neural Networks\n","\n","In this module, we will study:\n","1. Gradient Descent\n","2. Momentum-Based Gradient Descent\n","3. Nesterov Mementum\n","4. Adagrad\n","5. RMSProp\n","6. Adam\n"]},{"cell_type":"markdown","metadata":{"id":"zaGbgMHt16DS"},"source":["## Optimization Algorithms\n","\n","**What is optimization?**\n","\n","In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function.\n","\n","In the case of Machine Learning or Deep Learning, optimization refers to _the process of minimizing the loss function by systematically updating the network weights_. \n","\n","Mathematically, this is expressed as follows:\n","\n","\\begin{equation}\n","w = argmin_wL(w),\n","\\end{equation}\n","\n","where $L(w)$ and $w$ denotes, respectively, the loss function and weights.\n","\n","***\n","\n","**The Task of an Optimization Algorithm:**\n","\n","Optimization algorithms (in the case of minimization) have one of the following goals:\n","1. Find the global minimum of the objective function. This is feasible if the objective function is convex, i.e. any local minimum is a global minimum.\n","2. Find the lowest possible value of the objective function within its neighborhood. That’s usually the case if the objective function is not convex as the case in most deep learning problems.\n","\n","There are several optimization techniques, we will, in this module, learn the important once.\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"TURMOyXt3SbC"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"LwW3C2TH0p9h"},"source":["## References\n","\n","1. [A journey into Optimization algorithms for Deep Neural Networks](https://theaisummer.com/optimization/)\n","2. [The Hitchhiker’s Guide to Optimization in Machine Learning](https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-machine-learning-edcf5a104210)\n","3. [Gradient Descent Explained](https://towardsdatascience.com/gradient-descent-explained-9b953fc0d2c)\n","4. [Gentle Introduction to the Adam Optimization Algorithm for Deep Learning](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)"]},{"cell_type":"code","metadata":{"id":"b5n4RTu3wmfI"},"source":[""],"execution_count":null,"outputs":[]}]}