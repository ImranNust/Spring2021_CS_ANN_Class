{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Module2_RegularizationTechniquesForDeepNeuralNetworks.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"3rtxEC-Rwrvv"},"source":["# RegularizationTechniquesForDeepNeuralNetworks\n","\n","In this module, we will study:\n","1. Overfitting and Underfitting\n","2. Regularization Techniques\n","3. The Biase-Variance Tradeoff\n","4. L1 Regularization\n","5. L2 Regularization\n","6. Dropout\n","7. Early Stopping\n","8. Batch Normalization\n","9. And much more\n","\n","\n","![Image1](Module2Pic1.png)\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"AJ-80ccAgIOu"},"source":["# Regularization\n","\n","Regularization is a set of strategies used in Machine Learning to reduce the generalization error. Most models, after training, perform very well on a specific subset of the overall population but fail to generalize well. This is also known as overfitting. Regularization strategies aim to reduce overfitting and keep, at the same time, the training error as low as possible."]},{"cell_type":"markdown","metadata":{"id":"b5LdjmiNgU_-"},"source":["***\n","## The bias-variance tradeoff: overfitting and underfitting\n","\n","![Image1](Module2Pic2.png)"]},{"cell_type":"markdown","metadata":{"id":"eq17Qdr8g1X_"},"source":["### What is biase?\n","\n","Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wRT79Y8Wg6Tf"},"source":["### What is variance?\n","\n","Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data."]},{"cell_type":"markdown","metadata":{"id":"PgTg9DMyhQr7"},"source":["### What is underfitting?\n","\n","In supervised learning, underfitting happens when a model unable to capture the underlying pattern of the data. These models usually have high bias and low variance. It happens when we have very less amount of data to build an accurate model or when we try to build a linear model with a nonlinear data. Also, these kind of models are very simple to capture the complex patterns in data like Linear and logistic regression."]},{"cell_type":"markdown","metadata":{"id":"vA5pasCShch_"},"source":["### What is overfitting?\n","\n","In supervised learning, overfitting happens when our model captures the noise along with the underlying pattern in data. It happens when we train our model a lot over noisy dataset. These models have low bias and high variance. These models are very complex like Decision trees which are prone to overfitting.\n","\n","![Pic](Module2Pic3.png)\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"V95XU-K-hxfv"},"source":["### What is Bias Variance Tradeoff?\n","\n","The bias-variance tradeoff is a term to describe the fact that we can reduce the variance by increasing the bias. Good regularization techniques strive to simultaneously minimize the two sources of error. Hence, achieving better generalization.\n","\n","There are many regularization techniques, we will study a few of them.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6d9KqERHkd0C"},"source":["## L2 Regularization\n","\n","L2 regularization, also known as weight decay or ridge regression, adds a norm penalty in the form of $\\frac{1}{2}||w||_2^2$. As a result the cost function reduces to:\n","\\begin{equation}\n","J^\\prime(w; X, y) = J(w; X, y) + \\frac{\\alpha}{2}||w||_2^2\n","\\end{equation}\n","where $\\alpha$ is the regularization parameter between 0 and 1.\n","\n","If we compute the gradient w.r.t. $w$, the above equation reduces to:\n","\\begin{equation}\n","\\frac{\\partial}{\\partial w}J^\\prime(w; X, y) = \\frac{\\partial}{\\partial w}J(w; X, y) + \\alpha w\n","\\end{equation}\n","\n","The equation effectively shows us that each weight of the weight vector will be reduced by a constant factor on each training step.\n","\n","**Please note that we usually regularize only weights not biases**\n","\n","The L2 regularizer will have a big impact on the directions of the weight vector that don’t “contribute” much to the loss function. On the other hand, it will have a relatively small effect on the directions that contribute to the loss function. As a result, we reduce the variance of our model, which makes it easier to generalize on unseen data.\n"]},{"cell_type":"markdown","metadata":{"id":"_t3ufyIAn8Mk"},"source":["#### L2 Regularization in Keras\n","\n","In Keras, L2 regularization can be implemented as follows:\n","```\n","tf.keras.regularizers.l2(l2=0.01, **kwargs)\n","```\n","\n","L2 may be passed to a layer as a string identifier:\n","\n","```\n","l2 = tf.keras.regularizers.l2(l2=0.01)\n","dense = tf.keras.layers.Dense(3, kernel_regularizer=l2)\n","```\n","***"]},{"cell_type":"markdown","metadata":{"id":"s1Wyy0AIn52D"},"source":["## L1 Regularization\n","\n","L1 regularization, also known as Lasso regression, adds a norm penalty in the form of $\\frac{1}{2}||w||$. As a result the cost function reduces to:\n","\\begin{equation}\n","J^\\prime(w; X, y) = J(w; X, y) + \\frac{\\alpha}{2}||w||\n","\\end{equation}\n","where $\\alpha$ is the regularization parameter between 0 and 1.\n","\n","If we compute the gradient w.r.t. $w$, the above equation reduces to:\n","\\begin{equation}\n","\\frac{\\partial}{\\partial w}J^\\prime(w; X, y) = \\frac{\\partial}{\\partial w}J(w; X, y) + \\alpha\n","\\end{equation}\n","\n","As we can see, the regularization term does not scale linearly, contrary to L2 regularization, but it’s a constant factor.\n","\n","The L1 regularizer introduces sparsity in the weights by forcing more weights to be zero instead of reducing the average magnitude of all weights ( as the L2 regularizer does). In other words, L1 suggests that some features should be discarded whatsoever from the training process.\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"_m5mqHfEplbk"},"source":["#### L1 Regularization in Keras\n","\n","In Keras, L1 regularization can be implemented as follows:\n","```\n","tf.keras.regularizers.l1(l1=0.01, **kwargs)\n","```\n","\n","L1 may be passed to a layer as a string identifier:\n","\n","```\n","l1 = tf.keras.regularizers.l1(l1=0.01)\n","dense = tf.keras.layers.Dense(3, kernel_regularizer=l1)\n","```\n","***"]},{"cell_type":"markdown","metadata":{"id":"N356y3tg7iN-"},"source":["## Elastic Net Regularization\n","\n","we know the following:\n","\n","- L1 regularization produces sparse models, but cannot handle “small and fat datasets”.\n","- L2 regularization can handle these datasets, but can get you into trouble in terms of model interpretability due to the fact that it does not produce the sparse solutions you may wish to find after all.\n","\n","Elastic net is a method that linearly combines L1 and L2 regularization with the goal to acquire the best of both worlds . More specifically the penalty term is as follows:\n","\n","\\begin{equation}\n","\\Omega(\\theta) = \\beta_1 ||w||_1 + \\beta_2||w||^2_2\n","​\\end{equation}\n"," \n","Elastic Net regularization reduces the effect of certain features, as L1 does, but at the same time, it does not eliminate them. So it combines feature elimination from L1 and feature coefficient reduction from the L2.\n"]},{"cell_type":"markdown","metadata":{"id":"Ia5dY-Yi8dTP"},"source":["#### Elastic Regularization in Keras\n","\n","In Keras, L1-L2 regularization can be implemented as follows:\n","\n","```\n","model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=tensorflow.keras.regularizers.l1_l2(l1=0.01, l2=0.01)))\n","```\n","***"]},{"cell_type":"markdown","metadata":{"id":"WPO6qDOO9DV-"},"source":["## Entropy Regularization\n","\n","Entropy regularization is another norm penalty method that applies to probabilistic models. It has also been used in different Reinforcement Learning techniques such as A3C and policy optimization techniques. Similarly to the previous methods, we add a penalty term to the loss function.\n","\n","If we assume that the model outputs a probability distribution $p(x)$, then the penalty term will be denoted as:\n","\\begin{equation}\n","\\Omega(X) = -\\sum p(x)\\log (p(x))\n","\\end{equation}\n","\n","The term “Entropy” has been taken from information theory and represents the average level of \"information\" inherent in the variable's possible outcomes. An equivalent definition of entropy is the expected value of the information of a variable.\n","\n","One very simple explanation of why it works is that it forces the probability distribution towards the uniform distribution to reduce variance.\n","\n","In the context of Reinforcement Learning, one can say that the entropy term added to the loss, promotes action diversity and allows better exploration of the environment. \n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"Xd2SOUdl9lvf"},"source":["## Label smoothing\n","\n","Noise injection is one of the most powerful regularization strategies. By adding randomness, we can reduce the variance of the models and lower the generalization error. The question is how and where do we inject noise?\n","\n","Label smoothing is a way of adding noise at the output targets, also known as labels. Let’s assume that we have a classification problem. In most of them, we use a form of cross-entropy loss such as $-\\sum_{c=1}^M y_{o,c}\\log(p_{o,c})$ and softmax to output the final probabilities.\n","\n","The target vector has the form of $[0, 1 , 0 , 0]$. Because of the way softmax is formulated: $ \\sigma (\\mathbf {z} )_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{K}e^{z_{j}}}}$ , it can never achieve an output of 1 or 0. The best he can do is something like $[0.0003, 0.999, 0.0003, 0.0003]$. As a result, the model will continue to be trained, pushing the output values as high and as low as possible. The model will never converge. That, of course, will cause overfitting.\n","\n","To address that, label smoothing replaces the hard 0 and 1 targets by a small margin. Specifically, 0 are replaced with $\\frac{\\epsilon}{ k-1}$  and 1 with $1-\\epsilon$, where $k$ is the number of classes.\n","\n","**You can provide `label_smoothing` parameter at the time you define loss function in Keras.**\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"DXJNg3EF_pYf"},"source":["## Dropout\n","\n","Another strategy to regularize deep neural networks is dropout. Dropout falls into noise injection techniques and can be seen as noise injection into the hidden units of the network.\n","\n","In practice, during training, some number of layer outputs are randomly ignored (dropped out) with probability $p$.\n","\n","During test time, all units are present, but they have been scaled down by $p$. This is happening because after dropout, the next layers will receive lower values. In the test phase though, we are keeping all units so the values will be a lot higher than expected. That’s why we need to scale them down.\n","\n","By using dropout, the same layer will alter its connectivity and will search for alternative paths to convey the information in the next layer. As a result, each update to a layer during training is performed with a different “view” of the configured layer. Conceptually, it approximates training a large number of neural networks with different architectures in parallel.\n","\n","\"Dropping\" values means temporarily removing them from the network for the current forward pass, along with all its incoming and outgoing connections. Dropout has the effect of making the training process noisy. The choice of the probability $p$ depends on the architecture.\n","\n","![Pic](Module2Image4.png)\n","\n","This conceptualization suggests that perhaps dropout breaks up situations where network layers co-adapt to correct mistakes from prior layers, making the model more robust. It increases the sparsity of the network and in general, encourages sparse representations! Sparsity can be added to any model with hidden units and is a powerful tool in our regularization arsenal.\n","\n","***\n","\n","**Dropout layer can be implemented in Keras using the Dropout class as follows: `tf.keras.layers.Dropout(rate, noise_shape=None, seed=None)`**\n","***"]},{"cell_type":"markdown","metadata":{"id":"z5wAYJV4BbWB"},"source":["There are several variations of dropout techniques."]},{"cell_type":"markdown","metadata":{"id":"QrGU0zsBDXq5"},"source":["\n","## Early stopping\n","Early stopping is one of the most commonly used strategies because it is very simple and quite effective. It refers to the process of stopping the training when the training error is no longer decreasing but the validation error is starting to rise.\n","\n","![early-stopping](Module2Image5.png)\n","\n","\n","This implies that we store the trainable parameters periodically and track the validation error. After the training stopped, we return the trainable parameters to the exact point where the validation error started to rise, instead of the last ones.\n","\n","A different way to think of early stopping is as a very efficient hyperparameter selection algorithm, which sets the number of epochs to the absolute best. It essentially restricts the optimization procedure to a small volume of the trainable parameters space close to the initial parameters.\n","\n","It can also be proven that in the case of a simple linear model with a quadratic error function and simple gradient descent, early stopping is equivalent to L2 regularization.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dfmL-TgsD_Fz"},"source":["#### Early Stopping in Keras\n","\n","In Keras, EarlyStopping can be implemented using the following class:\n","```\n","tf.keras.callbacks.EarlyStopping(\n","    monitor=\"val_loss\",\n","    min_delta=0,\n","    patience=0,\n","    verbose=0,\n","    mode=\"auto\",\n","    baseline=None,\n","    restore_best_weights=False,\n",")\n","```\n","\n","**Example of EarlyStopping in Keras**\n","\n","```\n","callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n","# This callback will stop the training when there is no improvement in  the loss for three consecutive epochs.  \n","model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n","model.compile(tf.keras.optimizers.SGD(), loss='mse')\n","history = model.fit(np.arange(100).reshape(5, 20), \n","  np.zeros(5),epochs=10, batch_size=1, \n","  callbacks=[callback],        \n","  verbose=0)\n","len(history.history['loss'])  \n","```\n","***"]},{"cell_type":"markdown","metadata":{"id":"GZhcFsF7D_Zy"},"source":["## Parameter sharing\n","\n","Parameter sharing follows a different approach. Instead of penalizing model parameters, it forces a group of parameters to be equal. This can be seen as a way to apply our previous domain knowledge to the training process. Various approaches have been proposed over the years but the most popular one is by far Convolutional Neural Networks.\n","\n","Convolutional Neural Networks take advantage of the spatial structure of images by sharing parameters across different locations in the input. Since each kernel is convoluted with different blocks of the input image, the weight is shared among the blocks instead of having separate ones.\n","\n","![parameter-sharing](Module2Image6.png)\n","\n","***\n"]},{"cell_type":"markdown","metadata":{"id":"O0986I1nE-3B"},"source":["## Batch normalization\n","\n","Batch normalization (BN) can also be used as a form of regularization. Batch normalization fixes the means and variances of the input by bringing the feature in the same range. More specifically, we concentrate the features in a compact Gaussian-like space.\n","\n","Visually this can be represented as:\n","\n","![pic](Module2Pic7.png)\n","While mathematically we have:\n","\\begin{equation}\n","\\mu_{\\mathcal{B}} = \\frac{1}{m}\\sum^{m}_{i=1}x_{i}\n","\\end{equation}\n","\n","\\begin{equation}\n","\\sigma^{2}_{\\mathcal{B}} = \\frac{1}{m}\\sum^{m}_{i=1}\\left(x_{i}-\\mu_{\\mathcal{B}}\\right)^{2}\n","\\end{equation}\n","\n","\\begin{equation}\n","\\hat{x}_{i} = \\frac{x_{i} - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^{2}_{\\mathcal{B}}+\\epsilon}}\n","\\end{equation}\n","​\n","\\begin{equation}\n","y_{i} = \\gamma\\hat{x}_{i} + \\beta = \\text{BN}_{\\gamma, \\beta}\\left(x_{i}\\right)\n","\\end{equation}\n","\n","Where $\\gamma$ and $\\beta$ are learnable parameters.\n","***\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ev4KffBXGe89"},"source":["#### BatchNormalization in Keras\n","\n","In Keras, it can be implemented as follows:\n","```\n","BatchNormalization class\n","tf.keras.layers.BatchNormalization(\n","    axis=-1,\n","    momentum=0.99,\n","    epsilon=0.001,\n","    center=True,\n","    scale=True,\n","    beta_initializer=\"zeros\",\n","    gamma_initializer=\"ones\",\n","    moving_mean_initializer=\"zeros\",\n","    moving_variance_initializer=\"ones\",\n","    beta_regularizer=None,\n","    gamma_regularizer=None,\n","    beta_constraint=None,\n","    gamma_constraint=None,\n","    **kwargs\n",")\n","```\n","**Explanation**\n","\n","Keras provides support for batch normalization via the BatchNormalization layer.\n","\n","For example,\n","```\n","bn = BatchNormalization()\n","````\n","\n","The layer will transform inputs so that they are standardized, meaning that they will have a mean of zero and a standard deviation of one.\n","\n","During training, the layer will keep track of statistics for each input variable and use them to standardize the data.\n","\n","\n","***\n","\n","Further, the standardized output can be scaled using the learned parameters of $\\beta$ and $\\gamma$ that define the new mean and standard deviation for the output of the transform. The layer can be configured to control whether these additional parameters will be used or not via the “center” and “scale” attributes respectively. By default, they are enabled.\n","\n","***\n","The statistics used to perform the standardization, e.g. the mean and standard deviation of each variable, are updated for each mini batch and a running average is maintained.\n","***\n","\n","A “momentum” argument allows you to control how much of the statistics from the previous mini batch to include when the update is calculated. By default, this is kept high with a value of $0.99$. This can be set to $0.0$ to only use statistics from the current mini-batch, as described in the original paper.\n","```\n","bn = BatchNormalization(momentum=0.0)\n","```\n","At the end of training, the mean and standard deviation statistics in the layer at that time will be used to standardize inputs when the model is used to make a prediction.\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"4C7PELOlIclU"},"source":["## Data augmentation\n","\n","Data augmentation refers to the process of generating new training examples to our dataset. More training data means lower model’s variance, a.k.a lower generalization error. Simple as that. It can also be seen as a form of noise injection in the training dataset.\n","\n","Data augmentation can be achieved in many different ways. Let’s explore some of them.\n","\n","1. **Basic Data Manipulations:** The first simple thing to do is to perform geometric transformations on data. Most notably, if we’re talking about images we have solutions such as: Image flipping, cropping, rotations, translations, image color modification, image mixing etc. Cutout is a commonly used idea where we remove certain image regions. Another idea, called Mixup, is the process of blending two images from the dataset into one image.\n","\n","2. **Feature Space Augmentation:** Instead of transforming data in the input space as above, we can apply transformations on the feature space. For example, an autoencoder might be used to extract the latent representation. Noise can then be added in the latent representation which results in a transformation of the original data point.\n","\n","3. **GAN-based Augmentation:** Generative Adversarial Networks have been proven to work extremely well on data generation so they are a natural choice for data augmentation.\n","\n","4. **Meta-Learning:** In meta-learning, we use neural networks to optimize other neural networks by tuning their hyperparameters, improving their layout, and more. A similar approach can also be applied in data augmentation. In simple terms, we use a classification network to tune an augmentation network into generating better images. Example: We feed random images to an Augmentation Network (most likely a GAN), which will generate augmented images. Both the augmented image and the original are passed into a second network, which compares them and tells us how good the augmented image is. After repeating the process the augmentation network becomes better and better at producing new images."]},{"cell_type":"markdown","metadata":{"id":"LwW3C2TH0p9h"},"source":["## References\n","\n","1. [Regularization techniques for training deep neural networks](https://theaisummer.com/regularization/)\n","2. [Understanding the Bias-Variance Tradeoff](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)\n","3. [L1 and L2 Regularization — Explained](https://towardsdatascience.com/l1-and-l2-regularization-explained-874c3b03f668#:~:text=L2%20regularization%20forces%20weights%20toward,never%20be%20equal%20to%20zero.)\n","4. [Over-fitting and Regularization](https://towardsdatascience.com/over-fitting-and-regularization-64d16100f45c)\n","5. [L1 vs L2 Regularization](https://medium.com/analytics-vidhya/l1-vs-l2-regularization-which-is-better-d01068e6658c)\n","6. [Label Smoothing: An ingredient of higher model accuracy](https://towardsdatascience.com/label-smoothing-making-model-robust-to-incorrect-labels-2fae037ffbd0)\n","7. [Label smoothing with Keras, TensorFlow, and Deep Learning](https://www.pyimagesearch.com/2019/12/30/label-smoothing-with-keras-tensorflow-and-deep-learning/)\n","8. [Dropout Neural Network Layer In Keras Explained](https://towardsdatascience.com/machine-learning-part-20-dropout-keras-layers-explained-8c9f6dc4c9ab)\n","9. [Understanding Dropout with the Simplified Math behind it](https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275)\n","10. [Dropout in (Deep) Machine learning](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)"]},{"cell_type":"code","metadata":{"id":"b5n4RTu3wmfI"},"source":[""],"execution_count":null,"outputs":[]}]}