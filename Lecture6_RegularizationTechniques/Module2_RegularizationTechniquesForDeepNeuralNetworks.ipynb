{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Module2_RegularizationTechniquesForDeepNeuralNetworks.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"3rtxEC-Rwrvv"},"source":["# RegularizationTechniquesForDeepNeuralNetworks\n","\n","In this module, we will study:\n","1. Overfitting and Underfitting\n","2. Regularization Techniques\n","3. The Biase-Variance Tradeoff\n","4. L1 Regularization\n","5. L2 Regularization\n","6. Dropout\n","7. Early Stopping\n","8. Batch Normalization\n","9. And much more\n","\n","\n","![Image1](Module2Pic1.png)\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"AJ-80ccAgIOu"},"source":["# Regularization\n","\n","Regularization is a set of strategies used in Machine Learning to reduce the generalization error. Most models, after training, perform very well on a specific subset of the overall population but fail to generalize well. This is also known as overfitting. Regularization strategies aim to reduce overfitting and keep, at the same time, the training error as low as possible."]},{"cell_type":"markdown","metadata":{"id":"b5LdjmiNgU_-"},"source":["***\n","## The bias-variance tradeoff: overfitting and underfitting\n","\n","![Image1](Module2Pic2.png)"]},{"cell_type":"markdown","metadata":{"id":"eq17Qdr8g1X_"},"source":["### What is biase?\n","\n","Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wRT79Y8Wg6Tf"},"source":["### What is variance?\n","\n","Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data."]},{"cell_type":"markdown","metadata":{"id":"PgTg9DMyhQr7"},"source":["### What is underfitting?\n","\n","In supervised learning, underfitting happens when a model unable to capture the underlying pattern of the data. These models usually have high bias and low variance. It happens when we have very less amount of data to build an accurate model or when we try to build a linear model with a nonlinear data. Also, these kind of models are very simple to capture the complex patterns in data like Linear and logistic regression."]},{"cell_type":"markdown","metadata":{"id":"vA5pasCShch_"},"source":["### What is overfitting?\n","\n","In supervised learning, overfitting happens when our model captures the noise along with the underlying pattern in data. It happens when we train our model a lot over noisy dataset. These models have low bias and high variance. These models are very complex like Decision trees which are prone to overfitting.\n","\n","![Pic](Module2Pic3.png)\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"V95XU-K-hxfv"},"source":["### What is Bias Variance Tradeoff?\n","\n","The bias-variance tradeoff is a term to describe the fact that we can reduce the variance by increasing the bias. Good regularization techniques strive to simultaneously minimize the two sources of error. Hence, achieving better generalization.\n","\n","There are many regularization techniques, we will study a few of them.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6d9KqERHkd0C"},"source":["## L2 Regularization\n","\n","L2 regularization, also known as weight decay or ridge regression, adds a norm penalty in the form of $\\frac{1}{2}||w||_2^2$. As a result the cost function reduces to:\n","\\begin{equation}\n","J^\\prime(w; X, y) = J(w; X, y) + \\frac{\\alpha}{2}||w||_2^2\n","\\end{equation}\n","where $\\alpha$ is the regularization parameter between 0 and 1.\n","\n","If we compute the gradient w.r.t. $w$, the above equation reduces to:\n","\\begin{equation}\n","\\frac{\\partial}{\\partial w}J^\\prime(w; X, y) = \\frac{\\partial}{\\partial w}J(w; X, y) + \\alpha w\n","\\end{equation}\n","\n","The equation effectively shows us that each weight of the weight vector will be reduced by a constant factor on each training step.\n","\n","**Please note that we usually regularize only weights not biases**\n","\n","The L2 regularizer will have a big impact on the directions of the weight vector that don’t “contribute” much to the loss function. On the other hand, it will have a relatively small effect on the directions that contribute to the loss function. As a result, we reduce the variance of our model, which makes it easier to generalize on unseen data.\n"]},{"cell_type":"markdown","metadata":{"id":"LwW3C2TH0p9h"},"source":["## References\n","\n","1. [Regularization techniques for training deep neural networks](https://theaisummer.com/regularization/)\n","2. [Understanding the Bias-Variance Tradeoff](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)\n","3. [L1 and L2 Regularization — Explained](https://towardsdatascience.com/l1-and-l2-regularization-explained-874c3b03f668#:~:text=L2%20regularization%20forces%20weights%20toward,never%20be%20equal%20to%20zero.)\n","4. [Over-fitting and Regularization](https://towardsdatascience.com/over-fitting-and-regularization-64d16100f45c)\n","5. [L1 vs L2 Regularization](https://medium.com/analytics-vidhya/l1-vs-l2-regularization-which-is-better-d01068e6658c)\n"]},{"cell_type":"code","metadata":{"id":"b5n4RTu3wmfI"},"source":[""],"execution_count":null,"outputs":[]}]}