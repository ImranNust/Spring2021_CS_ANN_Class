{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Metrics](https://keras.io/api/metrics/)\n",
    "\n",
    "A metric is a function that is used to judge the performance of your model.\n",
    "\n",
    "Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. Note that you may use any loss function as a metric.\n",
    "\n",
    "Available metrics in the keras library are\n",
    "1. Accuracy metrics\n",
    "    1. Accuracy class\n",
    "    2. BinaryAccuracy class\n",
    "    3. CategoricalAccuracy class\n",
    "    4. TopKCategoricalAccuracy class\n",
    "    5. SparseTopKCategoricalAccuracy class\n",
    "2. Probabilistic metrics\n",
    "    1. BinaryCrossentropy class\n",
    "    2. CategoricalCrossentropy class\n",
    "    3. SparseCategoricalCrossentropy class\n",
    "    4. KLDivergence class\n",
    "    5. Poisson class\n",
    "3. Regression metrics\n",
    "    1. MeanSquaredError class\n",
    "    2. RootMeanSquaredError class\n",
    "    3. MeanAbsoluteError class\n",
    "    4. MeanAbsolutePercentageError class\n",
    "    5. MeanSquaredLogarithmicError class\n",
    "    6. CosineSimilarity class\n",
    "    7. LogCoshError class\n",
    "4. Classification metrics based on True/False positives & negatives\n",
    "    1. AUC class\n",
    "    2. Precision class\n",
    "    3. Recall class\n",
    "    4. TruePositives class\n",
    "    5. TrueNegatives class\n",
    "    6. FalsePositives class\n",
    "    7. FalseNegatives class\n",
    "    8. PrecisionAtRecall class\n",
    "    9. SensitivityAtSpecificity class\n",
    "    10. SpecificityAtSensitivity class\n",
    "5. Image segmentation metrics\n",
    "    1. MeanIoU class\n",
    "6. Hinge metrics for \"maximum-margin\" classification\n",
    "    1. Hinge class\n",
    "    2. SquaredHinge class\n",
    "    3. CategoricalHinge class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Introduction\n",
    "\n",
    "Choosing the right metric is crucial while evaluating machine learning (ML) models; various metrics are proposed to evaluate ML models in different applications. In some applications looking at a single metric may not give you the whole picture of the problem you are solving, and you may want to use a subset of the metrics. We will discuss, a few of the metrics, but remember, there are other metrics exist too.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Difference Between Metric and Loss Function\n",
    "\n",
    "It is also worth mentioning that metric is different from loss function. Loss functions are functions that show a measure of the model performance and are used to train a machine learning model (using some kind of optimization), and are usually **differentiable** in model’s parameters. On the other hand, metrics are used to monitor and measure the performance of a model (during training, and test), and do not need to be differentiable. However if for some tasks the performance metric is differentiable, it can be used both as a loss function (perhaps with some regularizations added to it), and a metric, such as MSE.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix \n",
    "\n",
    "**Remember that confusion matrix is not a mteric, but it is an important concept to learn.**\n",
    "\n",
    "One of the key concept in classification performance is confusion matrix, also known as error matrix, which is a tabular visualization of the model predictions versus the ground-truth labels. Each row of confusion matrix represents the instances in a predicted class and each column represents the instances in an actual class.\n",
    "\n",
    "Let’s go through this with an example. Let’s assume we are building a binary classification to classify cat images from non-cat images. And let’s assume our test set has 1100 images (1000 non-cat images, and 100 cat images), with the below confusion matrix.\n",
    "\n",
    "![ConfusionMatrix](image6.png)\n",
    "\n",
    "\n",
    "- **Out of 100 cat images**, the model has predicted 90 of them correctly  and has mis-classified 10 of them. If we refer to the “cat” class as positive and the non-cat class as negative class, then 90 samples predicted as cat are considered as as true-positive, and the 10 samples predicted as non-cat are false negative.\n",
    "- **Out of 1000 non-cat images**, the model has classified 940 of them correctly, and mis-classified 60 of them. The 940 correctly classified samples are referred as true-negative, and those 60 are referred as false-positive.\n",
    "\n",
    "As we can see diagonal elements of this matrix denote the correct prediction for different classes, while the off-diagonal elements denote the samples which are mis-classified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Classification metrics based on True/False positives & negatives\n",
    "\n",
    "### Classification Accuracy\n",
    "\n",
    "Classification Accuracy is measured using the relationship\n",
    "\n",
    "\\begin{equation}\n",
    "Accuracy = \\frac{Number\\ of\\ Correct\\ Predictions}{Total\\ Number\\ of\\ Prediction}\n",
    "\\end{equation}\n",
    "\n",
    "In Keras, `tf.keras.metrics.Accuracy(name=\"accuracy\", dtype=None)` can be used to calculate it.\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision class\n",
    "\n",
    "There are many cases in which classification accuracy is not a good indicator of your model performance. \n",
    "\n",
    "One of these scenarios is when your class distribution is imbalanced (one class is more frequent than others). In this case, even if you predict all samples as the most frequent class you would get a high accuracy rate, which does not make sense at all (because your model is not learning anything, and is just predicting everything as the top class). \n",
    "\n",
    "For example in our cat vs non-cat classification above, if the model predicts all samples as non-cat, it would result in a 1000/1100= 90.9%. Therefore we need to look at class specific performance metrics too. **Precision** is one of such metrics, which is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "Precision= \\frac{True\\_Positive}{True\\_Positive+ False\\_Positive}\n",
    "\\end{equation}\n",
    "\n",
    "The precision of Cat and Non-Cat class in the above example can be calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "Precision\\_cat= \\frac{samples \\ correctly\\  predicted \\ cat}{samples\\ predicted\\ as\\ cat} = \\frac{90}{90+60} = 60\\% \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "Precision\\_NonCat= \\frac{940}{950}= 98.9\\%\n",
    "\\end{equation}\n",
    "\n",
    "As we can see the model has much higher precision in predicting non-cat samples, versus cats. This is not surprising, as model has seen more examples of non-cat images during training, making it better in classifying that class.\n",
    "\n",
    "\n",
    "#### In Code\n",
    "In keras, we can use `tf.keras.metrics.Precision(thresholds=None, top_k=None, class_id=None, name=None, dtype=None)`. Let's see a few examples:\n",
    "\n",
    "\n",
    "\n",
    "**Example 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666667"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n",
    "m.result().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Precision(top_k=2)\n",
    "m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4:**\n",
    "With `top_k=4`, it will calculate precision over $y\\_true[:4]$ and $y\\_pred[:4]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Precision(top_k=4)\n",
    "m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage with compile() API:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1'))\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Recall class\n",
    "\n",
    "Recall is another important metric, which is defined as the fraction of samples from a class which are correctly predicted by the model as shown below\n",
    "\n",
    "\\begin{equation}\n",
    "Recall= \\frac{True\\_Positive}{True\\_Positive + False\\_Negative}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, for our example above, the recall rate of cat and non-cat classes can be found as:\n",
    "\n",
    "\\begin{equation}\n",
    "Recall_cat= \\frac{90}{100}= 90\\%\\\\\n",
    "Recall_NonCat= \\frac{940}{1000}= 94\\%\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "AUC class\n",
    "TruePositives class\n",
    "TrueNegatives class\n",
    "FalsePositives class\n",
    "FalseNegatives class\n",
    "PrecisionAtRecall class\n",
    "SensitivityAtSpecificity class\n",
    "SpecificityAtSensitivity class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Code\n",
    "\n",
    "In keras, we can use `tf.keras.metrics.Recall(\n",
    "    thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n",
    ")`. Let's see a few examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Recall()\n",
    "m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Recall()\n",
    "m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3: Usage with compile() API:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Likewise, we can compute\n",
    "\n",
    "1. **TruePositives** class using\n",
    "`tf.keras.metrics.TruePositives(thresholds=None, name=None, dtype=None)`\n",
    "2. **TrueNegatives** class using \n",
    "`tf.keras.metrics.TrueNegatives(thresholds=None, name=None, dtype=None)`\n",
    "3. **FalsePositives** class using\n",
    "`tf.keras.metrics.FalsePositives(thresholds=None, name=None, dtype=None)`\n",
    "4. **FalseNegatives** class using\n",
    "`tf.keras.metrics.FalseNegatives(thresholds=None, name=None, dtype=None)`\n",
    "5. **PrecisionAtRecall** class using\n",
    "`tf.keras.metrics.PrecisionAtRecall(recall, num_thresholds=200, class_id=None, name=None, dtype=None)`\n",
    "\n",
    " It Computes best precision where recall is >= specified value.\n",
    " This metric creates four local variables, **true_positives, true_negatives, false_positives and false_negatives** that are used to compute the precision at the given recall. The threshold for the given recall value is computed and used to evaluate the corresponding precision.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "Depending on application, you may want to give higher priority to recall or precision. But there are many applications in which both recall and precision are important. Therefore, it is natural to think of a way to combine these two into a single metric. One popular metric which combines precision and recall is called F1-score, which is the harmonic mean of precision and recall defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "F1-score= \\frac{2*Precision*Recall}{Precision+Recall}\n",
    "\\end{equation}\n",
    "\n",
    "So for our classification example with the confusion matrix in Figure 1, the F1-score can be calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "F1\\_cat= \\frac{2*0.6*0.9}{0.6+0.9}= 72\\%\n",
    "\\end{equation}\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity and Specificity\n",
    "Sensitivity and specificity are two other popular metrics mostly used in medical and biology related fields, and are defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "Sensitivity = Recall= \\frac{TP}{TP+FN} \\\\\n",
    "Specificity = True Negative Rate= \\frac{TN}{TN+FP}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (In Code) SensitivityAtSpecificity class\n",
    "\n",
    "**SensitivityAtSpecificity** class can be computed using \n",
    "`tf.keras.metrics.SensitivityAtSpecificity(specificity, num_thresholds=200, class_id=None, name=None, dtype=None)`\n",
    "\n",
    "It computes best sensitivity where specificity is >= specified value: the sensitivity at a given specificity.\n",
    "\n",
    "**Sensitivity** measures the proportion of actual positives that are correctly identified as such $\\frac{tp}{tp + fn}$. Specificity measures the proportion of actual negatives that are correctly identified as such $\\frac{tn}{tn + fp}.\n",
    "\n",
    "This metric creates four local variables, **true_positives, true_negatives, false_positives and false_negatives** that are used to compute the sensitivity at the given specificity. The threshold for the given specificity value is computed and used to evaluate the corresponding sensitivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.SensitivityAtSpecificity(0.6)\n",
    "m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.SensitivityAtSpecificity(0)\n",
    "m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],sample_weight=[1, 1, 2, 2, 1])\n",
    "m.result().numpy()\n",
    "0.333333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3: Usage with compile() API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='mse',\n",
    "    metrics=[tf.keras.metrics.SensitivityAtSpecificity(0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Similarly, **SpecificityAtSensitivity** class can be computed using\n",
    "`tf.keras.metrics.SpecificityAtSensitivity(sensitivity, num_thresholds=200, class_id=None, name=None, dtype=None)`\n",
    "\n",
    "It computes best specificity where sensitivity is >= specified value.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "\n",
    "The receiver operating characteristic curve is the plot that shows the performance of a binary classifier as function of its cut-off threshold. It essentially shows the true positive rate (TPR) against the false positive rate (FPR) for various threshold values. \n",
    "\n",
    "**Explanation**\n",
    "\n",
    "Many of the classification models are probabilistic; that is, they predict the probability of a sample being a cat. They then compare that output probability with some cut-off threshold and if it is larger than the threshold they predict its label as cat, otherwise as non-cat. As an example, your model may predict the below probabilities for 4 sample images: $[0.45, 0.6, 0.7, 0.3]$. Then depending on the threshold values below, you will get different labels:\n",
    "\n",
    "\\begin{equation}\n",
    "cut\\dash off= 0.5: predicted-labels= [0,1,1,0] (default threshold) \\\\\n",
    "cut-off= 0.2: predicted-labels= [1,1,1,1] \\\\\n",
    "cut-off= 0.8: predicted-labels= [0,0,0,0]\\\\\n",
    "\\end{equation}\n",
    "\n",
    "As you can see by varying the threshold values, we will get completely different labels. And as you can imagine each of these scenarios would result in a different precision and recall (as well as TPR, FPR) rates.\n",
    "ROC curve essentially finds out the TPR and FPR for various threshold values and plots TPR against the FPR. A sample ROC curve is shown in the following figure.\n",
    "\n",
    "![ROC Curve](image7.png)\n",
    "\n",
    "As we can see from this example, the lower the cut-off threshold on positive class, the more samples predicted as positive class, i.e. higher true positive rate (recall) and also higher false positive rate (corresponding to the right side of this curve). Therefore, there is a trade-off between how high the recall could be versus how much we want to bound the error (FPR).\n",
    "\n",
    "ROC curve is a popular curve to look at overall model performance and pick a good cut-off threshold for the model.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC\n",
    "\n",
    "The area under the curve (AUC), is an aggregated measure of performance of a binary classifier on all possible threshold values (and therefore it is threshold invariant).\n",
    "\n",
    "AUC calculates the area under the ROC curve, and therefore it is between 0 and 1. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.\n",
    "\n",
    "![AUC](image8.png)\n",
    "\n",
    "\n",
    "On high-level, the higher the AUC of a model the better it is. But sometimes threshold independent measure is not what you want, e.g. you may care about your model recall and require that to be higher than 99% (while it has a reasonable precision or FPR). In that case, you may want to tune your model threshold such that it meets your minimum requirement on those metrics (and you may not care even if you model AUC is not too high).\n",
    "Therefore in order to decide how to evaluate your classification model performance, perhaps you want to have a good understanding of the business/problem requirement and the impact of low recall vs. low precision, and decide what metric to optimize for.\n",
    "\n",
    "From a practical standpoint, a classification model which outputs probabilities is preferred over a single label output, as it provides the flexibility of tuning the threshold such that it meets your minimum recall/precision requirements. Not all models provide this nice probabilistic outputs though, e.g. SVM does not provide a simple probability as an output (although it provides margin which can be used to tune the decision, but it is not as straightforward and interpretable as having output probabilities).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Code\n",
    "\n",
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.AUC(num_thresholds=3)\n",
    "m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n",
    "# threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]  \n",
    "# tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]  \n",
    "# tp_rate = recall = [1, 0.5, 0], fp_rate = [1, 0, 0]  \n",
    "# auc = ((((1+0.5)/2)*(1-0)) + (((0.5+0)/2)*(0-0))) = 0.75  \n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "We can find different parameters as follows:\n",
    "- m.thresholds will return threshold values.\n",
    "- m.true_positive will return the TP values.\n",
    "- and so on\n",
    "***\n",
    "\n",
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.AUC(num_thresholds=3)\n",
    "m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],\n",
    "               sample_weight=[1, 0, 0, 1])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3 (Usage with compile() API):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reports the AUC of a model outputing a probability.\n",
    "model.compile(optimizer='sgd',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "\n",
    "\n",
    "# Reports the AUC of a model outputing a logit.\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape = (2, ), activation='sigmoid'))\n",
    "model.compile(optimizer='sgd',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.AUC()]) # use `from_logits = True` here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## Regression Related Metrics\n",
    "\n",
    "Regression models are another family of machine learning and statistical models, which are used to predict a continuous target values³. They have a wide range of applications, from house price prediction, E-commerce pricing systems, weather forecasting, stock market prediction, to image super resolution, feature learning via auto-encoders, and image compression.\n",
    "\n",
    "Models such as linear regression, random forest, XGboost, convolutional neural network, recurrent neural network are some of the most popular regression models.\n",
    "\n",
    "Metrics used to evaluate these models should be able to work on a set of continuous values (with infinite cardinality), and are therefore slightly different from classification metrics.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanSquaredError class\n",
    "\n",
    "“Mean squared error” is perhaps the most popular metric used for regression problems. It essentially finds the average squared error between the predicted and actual values, and in keras, we can use \n",
    "\n",
    "`tf.keras.metrics.MeanSquaredError(name=\"mean_squared_error\", dtype=None)` \n",
    "\n",
    "to compute MSE.\n",
    "\n",
    "Let’s assume we have a regression model which predicts the price of houses in Seattle area (show them with $\\hat{y}_i$), and let’s say for each house we also have the actual price the house was sold for (denoted with $y_i$). Then the MSE can be calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "MSE = \\frac{1}{N}\\sum_{i=1}^{N}\\left(y_i - \\hat{y}_i\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "Sometimes people use **RMSE** to have a metric with scale as the target values, which is essentially the **square root of MSE**.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.MeanSquaredError()\n",
    "m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.MeanSquaredError()\n",
    "m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n",
    "               sample_weight=[1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3 (Usage with compile() API):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, activation = 'linear', input_shape = (2,)))\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='mse',\n",
    "    metrics=[tf.keras.metrics.MeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Similarly, the room mean squared error (RMSE) can be calculated, in keras, using \n",
    "\n",
    "`tf.keras.metrics.RootMeanSquaredError(name=\"root_mean_squared_error\", dtype=None)`.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanAbsoluteError class\n",
    "\n",
    "Mean absolute error (or mean absolute deviation) is another metric which finds the average absolute distance between the predicted and target values. MAE is define as below:\n",
    "\n",
    "\\begin{equation}\n",
    "MAE = \\frac{1}{N}\\sum_{i=1}^{N}\\left|y_i - \\hat{y}_i\\right|\n",
    "\\end{equation}\n",
    "\n",
    "**MAE is known to be more robust to the outliers than MSE.** The main reason being that in MSE by squaring the errors, the outliers (which usually have higher errors than other samples) get more attention and dominance in the final error and impacting the model parameters.\n",
    "\n",
    "It is also worth mentioning that there is a nice maximum likelihood (MLE) interpretation behind MSE and MAE metrics. If we assume a linear dependence between features and targets, then MSE and MAE correspond to the MLE on the model parameters by assuming Gaussian and Laplace priors on the model errors respectively.\n",
    "\n",
    "In Keras, we can use\n",
    "\n",
    "`tf.keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\", dtype=None)`\n",
    "\n",
    "to compute the MAE.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.MeanAbsoluteError()\n",
    "m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.MeanAbsoluteError()\n",
    "m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n",
    "               sample_weight=[1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-eed2f758280a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-eed2f758280a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    **Example 3 (Usage with compile() API):**\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "**Example 3 (Usage with compile() API):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, activation = 'linear', input_shape = (2,)))\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='mse',\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### [MeanSquaredLogarithmicError](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)) class\n",
    "\n",
    "The MSLE is define as \n",
    "\n",
    "\\begin{equation}\n",
    "L(y,\\hat{y}) = \\frac{1}{N}\\sum_{i=1}^{N}\\left(log(y_i+1)-log(\\hat{y}_i+1)\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "In Keras, we can use\n",
    " \n",
    "`tf.keras.metrics.MeanSquaredLogarithmicError(name=\"mean_squared_logarithmic_error\", dtype=None)`.\n",
    "\n",
    "**Use MSLE when doing regression, believing that your target, conditioned on the input, is normally distributed, and you don’t want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy class\n",
    "\n",
    "The accuracy can be computed using\n",
    "\n",
    "`tf.keras.metrics.Accuracy(name=\"accuracy\", dtype=None)` \n",
    "\n",
    "and it is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "Accuracy = \\frac{Total \\ Correct\\ Predictions}{Total\\ Number\\ of\\ Predictions}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2 (Usage with compile() API:):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.Accuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BinaryAccuracy class\n",
    "\n",
    "The Binary Accuracy can be computedas follows:\n",
    "\n",
    "`tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", dtype=None, threshold=0.5)`\n",
    "\n",
    "Calculates how often predictions match binary labels.\n",
    "\n",
    "This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true. This frequency is ultimately returned as binary accuracy: an idempotent operation that simply divides total by count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.BinaryAccuracy()\n",
    "m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.BinaryAccuracy()\n",
    "m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]],\n",
    "               sample_weight=[1, 0, 0, 1])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3 (Usage with compile() API):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Similary, we can compute\n",
    "\n",
    "1. **CategoricalAccuracy** using \n",
    "`tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None)`\n",
    "\n",
    "2. **TopKCategoricalAccuracy** using\n",
    "`tf.keras.metrics.TopKCategoricalAccuracy(k=5,name=\"top_k_categorical_accuracy\", dtype=None)`\n",
    "\n",
    "3. **SparseTopKCategoricalAccuracy** using\n",
    "`tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name=\"sparse_top_k_categorical_accuracy\", dtype=None)`\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic metrics\n",
    "### BinaryCrossentropy class\n",
    "\n",
    "BinaryCrossentropy can be computed using\n",
    "\n",
    "`tf.keras.metrics.BinaryCrossentropy(name=\"binary_crossentropy\", dtype=None, from_logits=False, label_smoothing=0)`\n",
    "\n",
    "which computes the crossentropy metric between the labels and predictions.\n",
    "\n",
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.BinaryCrossentropy()\n",
    "m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2 (Usage with compile() API):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='mse',\n",
    "    metrics=[tf.keras.metrics.BinaryCrossentropy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "Similary, we can compute\n",
    "\n",
    "1. **CategoricalCrossentropy** using\n",
    "\n",
    "`tf.keras.metrics.CategoricalCrossentropy(name=\"categorical_crossentropy\", dtype=None, from_logits=False, label_smoothing=0)`\n",
    "\n",
    "2. **SparseCategoricalCrossentropy** using\n",
    "\n",
    "`tf.keras.metrics.SparseCategoricalCrossentropy(name=\"sparse_categorical_crossentropy\", dtype=None, from_logits=False, axis=-1)`\n",
    "\n",
    "3. **KLDivergence** using\n",
    "\n",
    "`tf.keras.metrics.KLDivergence(name=\"kullback_leibler_divergence\", dtype=None) `\n",
    "\n",
    "4. **Poisson** using\n",
    "`tf.keras.metrics.Poisson(name=\"poisson\", dtype=None)`\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are other metrics as well, you can learn more about them by visiting the references given at the end of this tutorial.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Metrics](https://keras.io/api/metrics/)\n",
    "2. [20 Popular Machine Learning Metrics.](https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce)\n",
    "3. [Mean Squared Logarithmic Error](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle))\n",
    "4. [Accuracy Metrics](https://towardsdatascience.com/keras-accuracy-metrics-8572eb479ec7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda0153f39fba084bbcb5c9acd2b000e8a8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
