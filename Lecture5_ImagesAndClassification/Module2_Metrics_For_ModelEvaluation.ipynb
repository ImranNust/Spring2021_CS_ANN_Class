{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Metrics](https://keras.io/api/metrics/)\n",
    "\n",
    "A metric is a function that is used to judge the performance of your model.\n",
    "\n",
    "Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. Note that you may use any loss function as a metric.\n",
    "\n",
    "Available metrics in the keras library are\n",
    "1. Accuracy metrics\n",
    "    1. Accuracy class\n",
    "    2. BinaryAccuracy class\n",
    "    3. CategoricalAccuracy class\n",
    "    4. TopKCategoricalAccuracy class\n",
    "    5. SparseTopKCategoricalAccuracy class\n",
    "2. Probabilistic metrics\n",
    "    1. BinaryCrossentropy class\n",
    "    2. CategoricalCrossentropy class\n",
    "    3. SparseCategoricalCrossentropy class\n",
    "    4. KLDivergence class\n",
    "    5. Poisson class\n",
    "3. Regression metrics\n",
    "    1. MeanSquaredError class\n",
    "    2. RootMeanSquaredError class\n",
    "    3. MeanAbsoluteError class\n",
    "    4. MeanAbsolutePercentageError class\n",
    "    5. MeanSquaredLogarithmicError class\n",
    "    6. CosineSimilarity class\n",
    "    7. LogCoshError class\n",
    "4. Classification metrics based on True/False positives & negatives\n",
    "    1. AUC class\n",
    "    2. Precision class\n",
    "    3. Recall class\n",
    "    4. TruePositives class\n",
    "    5. TrueNegatives class\n",
    "    6. FalsePositives class\n",
    "    7. FalseNegatives class\n",
    "    8. PrecisionAtRecall class\n",
    "    9. SensitivityAtSpecificity class\n",
    "    10. SpecificityAtSensitivity class\n",
    "5. Image segmentation metrics\n",
    "    1. MeanIoU class\n",
    "6. Hinge metrics for \"maximum-margin\" classification\n",
    "    1. Hinge class\n",
    "    2. SquaredHinge class\n",
    "    3. CategoricalHinge class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Introduction\n",
    "\n",
    "Choosing the right metric is crucial while evaluating machine learning (ML) models; various metrics are proposed to evaluate ML models in different applications. In some applications looking at a single metric may not give you the whole picture of the problem you are solving, and you may want to use a subset of the metrics. We will discuss, a few of the metrics, but remember, there are other metrics exist too.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Difference Between Metric and Loss Function\n",
    "\n",
    "It is also worth mentioning that metric is different from loss function. Loss functions are functions that show a measure of the model performance and are used to train a machine learning model (using some kind of optimization), and are usually **differentiable** in model’s parameters. On the other hand, metrics are used to monitor and measure the performance of a model (during training, and test), and do not need to be differentiable. However if for some tasks the performance metric is differentiable, it can be used both as a loss function (perhaps with some regularizations added to it), and a metric, such as MSE.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix \n",
    "\n",
    "**Remember that confusion matrix is not a mteric, but it is an important concept to learn.**\n",
    "\n",
    "One of the key concept in classification performance is confusion matrix, also known as error matrix, which is a tabular visualization of the model predictions versus the ground-truth labels. Each row of confusion matrix represents the instances in a predicted class and each column represents the instances in an actual class.\n",
    "\n",
    "Let’s go through this with an example. Let’s assume we are building a binary classification to classify cat images from non-cat images. And let’s assume our test set has 1100 images (1000 non-cat images, and 100 cat images), with the below confusion matrix.\n",
    "\n",
    "![ConfusionMatrix](image6.png)\n",
    "\n",
    "\n",
    "- **Out of 100 cat images**, the model has predicted 90 of them correctly  and has mis-classified 10 of them. If we refer to the “cat” class as positive and the non-cat class as negative class, then 90 samples predicted as cat are considered as as true-positive, and the 10 samples predicted as non-cat are false negative.\n",
    "- **Out of 1000 non-cat images**, the model has classified 940 of them correctly, and mis-classified 60 of them. The 940 correctly classified samples are referred as true-negative, and those 60 are referred as false-positive.\n",
    "\n",
    "As we can see diagonal elements of this matrix denote the correct prediction for different classes, while the off-diagonal elements denote the samples which are mis-classified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Classification metrics based on True/False positives & negatives\n",
    "\n",
    "### Classification Accuracy\n",
    "\n",
    "Classification Accuracy is measured using the relationship\n",
    "\n",
    "\\begin{equation}\n",
    "Accuracy = \\frac{Number\\ of\\ Correct\\ Predictions}{Total\\ Number\\ of\\ Prediction}\n",
    "\\end{equation}\n",
    "\n",
    "In Keras, `tf.keras.metrics.Accuracy(name=\"accuracy\", dtype=None)` can be used to calculate it.\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision class\n",
    "\n",
    "There are many cases in which classification accuracy is not a good indicator of your model performance. \n",
    "\n",
    "One of these scenarios is when your class distribution is imbalanced (one class is more frequent than others). In this case, even if you predict all samples as the most frequent class you would get a high accuracy rate, which does not make sense at all (because your model is not learning anything, and is just predicting everything as the top class). \n",
    "\n",
    "For example in our cat vs non-cat classification above, if the model predicts all samples as non-cat, it would result in a 1000/1100= 90.9%. Therefore we need to look at class specific performance metrics too. **Precision** is one of such metrics, which is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "Precision= \\frac{True\\_Positive}{True\\_Positive+ False\\_Positive}\n",
    "\\end{equation}\n",
    "\n",
    "The precision of Cat and Non-Cat class in the above example can be calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "Precision\\_cat= \\frac{samples \\ correctly\\  predicted \\ cat}{samples\\ predicted\\ as\\ cat} = \\frac{90}{90+60} = 60\\% \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "Precision\\_NonCat= \\frac{940}{950}= 98.9\\%\n",
    "\\end{equation}\n",
    "\n",
    "As we can see the model has much higher precision in predicting non-cat samples, versus cats. This is not surprising, as model has seen more examples of non-cat images during training, making it better in classifying that class.\n",
    "\n",
    "\n",
    "#### In Code\n",
    "In keras, we can use `tf.keras.metrics.Precision(thresholds=None, top_k=None, class_id=None, name=None, dtype=None)`. Let's see a few examples:\n",
    "\n",
    "\n",
    "\n",
    "**Example 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666667"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n",
    "m.result().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Precision(top_k=2)\n",
    "m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4:**\n",
    "With `top_k=4`, it will calculate precision over $y\\_true[:4]$ and $y\\_pred[:4]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Precision(top_k=4)\n",
    "m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall class\n",
    "\n",
    "Recall is another important metric, which is defined as the fraction of samples from a class which are correctly predicted by the model as shown below\n",
    "\n",
    "\\begin{equation}\n",
    "Recall= \\frac{True\\_Positive}{True\\_Positive + False\\_Negative}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, for our example above, the recall rate of cat and non-cat classes can be found as:\n",
    "\n",
    "\\begin{equation}\n",
    "Recall_cat= \\frac{90}{100}= 90\\%\\\\\n",
    "Recall_NonCat= \\frac{940}{1000}= 94\\%\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "AUC class\n",
    "TruePositives class\n",
    "TrueNegatives class\n",
    "FalsePositives class\n",
    "FalseNegatives class\n",
    "PrecisionAtRecall class\n",
    "SensitivityAtSpecificity class\n",
    "SpecificityAtSensitivity class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Metrics](https://keras.io/api/metrics/)\n",
    "2. [20 Popular Machine Learning Metrics.](https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda0153f39fba084bbcb5c9acd2b000e8a8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
