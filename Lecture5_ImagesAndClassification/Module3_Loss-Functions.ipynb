{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Loss Functions](https://keras.io/api/losses/)\n",
    "\n",
    "The loss function quantifies how well a model is performing a task by calculating a single number, **the loss**, from the model output and the desired target.\n",
    "\n",
    "If the model predictions are totally wrong, the loss will be a high number. If they’re pretty good, it will be close to zero.\n",
    "\n",
    "You select the Loss function you want to use in the parameters of the Target block.\n",
    "\n",
    "During training, the optimizer tunes the model to minimize the loss on training examples. After at least 1 epoch has run, the loss and metrics plot of the Evaluation view will show the average value of the loss over all the training examples, as well as over the validation examples.\n",
    "\n",
    "\n",
    "![Loss Functions](image9.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing an appropriate loss function is an important step. The following table can be helpful in this regard:\n",
    "\n",
    "![ChoosingFunctions](image10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compatibility with activation functions**\n",
    "\n",
    "Some loss functions can only be calculated for a limited range of model outputs. You can ensure that the model output is always in the correct range by using an appropriate activation function on the last block of the model.\n",
    "\n",
    "**Example**\n",
    "\n",
    "The **categorical crossentropy** loss function needs to calculate the logarithm of the model prediction, which is only possible if the model output is strictly positive.\n",
    "\n",
    "1. The TanH activation outputs values between -1 and 1, which makes it incompatible with the categorical crossentropy.\n",
    "\n",
    "2. The sigmoid activation outputs values between 0 and 1, which makes it a perfect match for the categorical crossentropy!\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Losses\n",
    "\n",
    "In this section, we will learn loss functions used for classification.\n",
    "1. Categorical crossentropy\n",
    "2. Binary crossentropy\n",
    "3. Squared hinge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Crossentropy Loss Function\n",
    "\n",
    "Binary crossentropy is a loss function that is used in binary classification tasks. Mathemtically, it can be defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "Loss = -\\frac{1}{N}\\sum_{i=1}^{N}\\left(y_i. log\\hat{y}_i+(1-y_i).log(1-\\hat{y}_i)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y}_i$ and $y_i$ represents predicted and actual output for an example $i$.\n",
    "\n",
    "**Please note that sigmoid is the only activation function compatible with the binary crossentropy loss function.**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, we can either use **BinaryCrossentropy class** or **binary_crossentropy function**.\n",
    "***\n",
    "\n",
    "#### binary_crossentropy function\n",
    "\n",
    "Following code can be used to compute the binary_crossentropy using the binary_crossentropy function.\n",
    "```\n",
    "tf.keras.losses.binary_crossentropy(\n",
    "y_true, y_pred, from_logits=False,\n",
    "label_smoothing=0, axis=-1)\n",
    "```\n",
    "\n",
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9162905 , 0.71355796], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0, 1], [0, 0]]\n",
    "y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
    "loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "loss.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### BinaryCrossentropy class\n",
    "\n",
    "Following code can be used to compute the binary_crossentropy using the binary_crossentropy class.\n",
    "\n",
    "```\n",
    "tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=False,\n",
    "    label_smoothing=0,\n",
    "    axis=-1,\n",
    "    reduction=\"auto\",\n",
    "    name=\"binary_crossentropy\")\n",
    "```\n",
    "\n",
    "**_from_logits: when it is True, it means that the prediction result is not a probability distribution, but the exact category value; when it is False, it means that the output is a probability distribution_**\n",
    "\n",
    "**Example 1:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.865458"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1: (batch_size = 1, number of samples = 4)  \n",
    "y_true = [0, 1, 0, 0]\n",
    "y_pred = [-18.6, 0.51, 2.94, -12.8]\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "bce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.988211"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the following updates to the above \"Recommended Usage\" section  \n",
    "# 1. Set `from_logits=False`  \n",
    "tf.keras.losses.BinaryCrossentropy() # OR ...('from_logits=False')\n",
    "# 2. Update `y_pred` to use probabilities instead of logits  \n",
    "y_pred = [0.6, 0.3, 0.2, 0.8] # OR [[0.6, 0.3], [0.2, 0.8]]\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "bce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3 (With tf.keras API):**\n",
    "```\n",
    "model.compile(\n",
    "  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "  ....\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### [Categorical Crossentropy](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy)\n",
    "\n",
    "\n",
    "\n",
    "Categorical crossentropy is a loss function that is used in **multi-class classification** tasks. These are tasks where an example can only belong to one out of many possible categories, and the model must decide which one.\n",
    "\n",
    "The categorical crossentropy loss function calculates the loss of an example by computing the following sum:\n",
    "\n",
    "\\begin{equation}\n",
    "Loss = - \\sum_{i=1}^{N} y_i . log\\left(\\hat{y}_i\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y}_i$ and $y_i$ represents predicted and actual output for an example $i$.\n",
    "\n",
    "**Softmax is the only activation function recommended to use with the categorical crossentropy loss function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "In Keras, we can either use **BinaryCrossentropy class** or **binary_crossentropy function**.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CategoricalCrossentropy class\n",
    "\n",
    "Following code can be used to compute the Categorical Crossentropy using the CategoricalCrossentropy Class.\n",
    "\n",
    "```\n",
    "tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    label_smoothing=0,\n",
    "    axis=-1,\n",
    "    reduction=\"auto\",\n",
    "    name=\"categorical_crossentropy\",\n",
    ")\n",
    "```\n",
    "\n",
    "It computes the crossentropy loss between the labels and predictions.\n",
    "\n",
    "Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a **one_hot** representation. If you want to provide labels as integers, please use **SparseCategoricalCrossentropy** loss. \n",
    "\n",
    "**Example 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1769392"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0, 1, 0], [0, 0, 1]]\n",
    "y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.  \n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "cce(y_true, y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3538785"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 'sum' reduction type.  \n",
    "cce = tf.keras.losses.CategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.SUM)\n",
    "cce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05129331, 2.3025851 ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 'none' reduction type.  \n",
    "cce = tf.keras.losses.CategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE)\n",
    "cce(y_true, y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4 (Usage with the compile() API):**\n",
    "```\n",
    "model.compile(\n",
    "optimizer='sgd', \n",
    "loss=tf.keras.losses.CategoricalCrossentropy()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### categorical_crossentropy function\n",
    "\n",
    "Following code can be used to compute the binary_crossentropy using the binary_crossentropy function.\n",
    "```\n",
    "tf.keras.losses.categorical_crossentropy(\n",
    "    y_true, y_pred, from_logits=False, \n",
    "    label_smoothing=0, axis=-1\n",
    ")\n",
    "```\n",
    "\n",
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05129331, 2.3025851 ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0, 1, 0], [0, 0, 1]]\n",
    "y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Sparse Categorical Crossentropy](https://sanjivgautamofficial.medium.com/categorical-cross-entropy-vs-sparse-categorical-cross-entropy-b6a24de2b7f0)\n",
    "\n",
    "\n",
    "If your targets are one-hot encoded, use **Categoricall Crossentorpy** loss function.\n",
    "\n",
    "Examples of one-hot encodings:\n",
    "- $[1, 0, 0]$\n",
    "- $[0, 1, 0]$\n",
    "- $[0, 0, 1]$\n",
    "\n",
    "***\n",
    "\n",
    "But if your targets are integers, use **Sparse Categorical Crossentropy**\n",
    "\n",
    "Examples of integer encodings\n",
    "- $1$\n",
    "- $2$\n",
    "- $3$\n",
    "\n",
    "***\n",
    "\n",
    "Like binary crossentropy and categorical crossentropy, the Keras library has function and classes for Sparse Categorical Crossentorpies.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Loss Function\n",
    "\n",
    "The poisson loss function is used for regression when modeling count data. Use for data follows the poisson distribution. Ex: churn of customers next week. The loss takes the form of:\n",
    "\n",
    "\\begin{equation}\n",
    "L\\left(y, \\hat{y}\\right) = \\frac{1}{N} \\sum_{i=0}^{N}\\left({\\hat{y}}_i - y_ilog{\\hat{y}}_i\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "where $\\hat{y}$ is the predicted expected value.\n",
    "\n",
    "Minimizing the Poisson loss is equivalent of maximizing the likelihood of the data under the assumption that the target comes from a Poisson distribution, conditioned on the input.\n",
    "\n",
    "1. In Keras, Poisson Class can be used\n",
    "```\n",
    "tf.keras.losses.Poisson(reduction=\"auto\", name=\"poisson\")\n",
    "```\n",
    "2. In Keras, to compute Poisson loss, Poisson function be used as follows:\n",
    "```\n",
    "tf.keras.losses.poisson(y_true, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## Regression Losses\n",
    "\n",
    "Regression models are another family of machine learning and statistical models, which are used to predict a continuous target values³. They have a wide range of applications, from house price prediction, E-commerce pricing systems, weather forecasting, stock market prediction, to image super resolution, feature learning via auto-encoders, and image compression.\n",
    "\n",
    "Models such as linear regression, random forest, XGboost, convolutional neural network, recurrent neural network are some of the most popular regression models.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanSquaredError Loss\n",
    "\n",
    "Mean Squared Error (MSE) is perhaps the most popular metric used for regression problems. It essentially finds the average squared error between the predicted and actual values, and in keras, we can use \n",
    "\n",
    "`tf.keras.metrics.MeanSquaredError(name=\"mean_squared_error\", dtype=None)` \n",
    "\n",
    "to compute MSE.\n",
    "\n",
    "Let’s assume we have a regression model which predicts the price of houses in Seattle area (show them with $\\hat{y}_i$), and let’s say for each house we also have the actual price the house was sold for (denoted with $y_i$). Then the MSE can be calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "MSE = \\frac{1}{N}\\sum_{i=1}^{N}\\left(y_i - \\hat{y}_i\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "Sometimes people use **RMSE** to have a metric with scale as the target values, which is essentially the **square root of MSE**.\n",
    "\n",
    "**Why use mean squared error?**\n",
    "\n",
    "MSE is sensitive towards outliers and given several examples with the same input feature values, the optimal prediction will be their mean target value. This should be compared with Mean Absolute Error, where the optimal prediction is the median. MSE is thus good to use if you believe that your target data, conditioned on the input, is normally distributed around a mean value, and when it’s important to penalize outliers extra much.\n",
    "\n",
    "\n",
    "**When to use mean squared error?**\n",
    "\n",
    "Use MSE when doing regression, believing that your target, conditioned on the input, is normally distributed, and want large errors to be significantly (quadratically) more penalized than small ones.\n",
    "\n",
    "**Example:** You want to predict future house prices. The price is a continuous value, and therefore we want to do regression. MSE can here be used as the loss function.\n",
    "\n",
    "***\n",
    "\n",
    "We can use classes of functions, in Keras, to compute MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MeanSquaredError class\n",
    "\n",
    "Following code can be used to compute the MSE using the MeanSquaredError Class.\n",
    "\n",
    "```\n",
    "tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
    "````\n",
    "\n",
    "It computes the mean of squares of errors between labels and predictions.\n",
    "\n",
    "loss = square(y_true - y_pred)\n",
    "\n",
    "**Example 1**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[1., 1.], [1., 0.]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.  \n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mse(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling with 'sample_weight'.  \n",
    "mse(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 'sum' reduction type.  \n",
    "mse = tf.keras.losses.MeanSquaredError(\n",
    "    reduction=tf.keras.losses.Reduction.SUM)\n",
    "mse(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 'none' reduction type.  \n",
    "mse = tf.keras.losses.MeanSquaredError(\n",
    "    reduction=tf.keras.losses.Reduction.NONE)\n",
    "mse(y_true, y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5 (Usage with the compile() API):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, activation = 'linear', input_shape = (2,)))\n",
    "model.compile(optimizer='sgd', \n",
    "              loss=tf.keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Similarly, the room mean squared error (RMSE) can be calculated, in keras, using \n",
    "\n",
    "`tf.keras.losses.RootMeanSquaredError(name=\"root_mean_squared_error\", dtype=None)`.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean_squared_error function\n",
    "\n",
    "```\n",
    "tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28208511, 0.44002081])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.random.randint(0, 2, size=(2, 3))\n",
    "y_pred = np.random.random(size=(2, 3))\n",
    "loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanAbsoluteError Loss\n",
    "\n",
    "Mean absolute error (or mean absolute deviation) is another loss function that finds the average absolute distance between the predicted and target values. MAE is define as below:\n",
    "\n",
    "\\begin{equation}\n",
    "MAE = \\frac{1}{N}\\sum_{i=1}^{N}\\left|y_i - \\hat{y}_i\\right|\n",
    "\\end{equation}\n",
    "\n",
    "**MAE is known to be more robust to the outliers than MSE.** The main reason being that in MSE by squaring the errors, the outliers (which usually have higher errors than other samples) get more attention and dominance in the final error and impacting the model parameters.\n",
    "\n",
    "It is also worth mentioning that there is a nice maximum likelihood (MLE) interpretation behind MSE and MAE metrics. If we assume a linear dependence between features and targets, then MSE and MAE correspond to the MLE on the model parameters by assuming Gaussian and Laplace priors on the model errors respectively.\n",
    "\n",
    "\n",
    "**Why use mean absolute error?**\n",
    "\n",
    "MAE is not sensitive towards outliers and given several examples with the same input feature values, and the optimal prediction will be their median target value. This should be compared with Mean Squared Error, where the optimal prediction is the mean. A disadvantage of MAE is that the gradient magnitude is not dependent on the error size, only on the sign of $y - \\hat{y}$. This leads to that the gradient magnitude will be large even when the error is small, which in turn can lead to convergence problems.\n",
    "\n",
    "**When to use mean absolute error?**\n",
    "\n",
    "Use Mean absolute error when you are doing regression and don’t want outliers to play a big role. It can also be useful if you know that your distribution is multimodal, and it’s desirable to have predictions at one of the modes, rather than at the mean of them.\n",
    "\n",
    "Example: When doing image reconstruction, MAE encourages less blurry images compared to MSE. This is used for example in the paper Image-to-Image Translation with Conditional Adversarial Networks by Isola et al.\n",
    "\n",
    "In Keras, we can use **MeanAbsoluteError class** as follows:\n",
    "\n",
    "```\n",
    "tf.keras.losses.MeanAbsoluteError(\n",
    "    reduction=\"auto\", name=\"mean_absolute_error\"\n",
    ")\n",
    "```\n",
    "\n",
    "In Keras, we can use **mean_absolute_error function** as follows:\n",
    "\n",
    "```\n",
    "tf.keras.losses.mean_absolute_error(y_true, y_pred)\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[1., 1.], [1., 0.]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.  \n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "mae(y_true, y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling with 'sample_weight'.  \n",
    "mae(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 'sum' reduction type.  \n",
    "mae = tf.keras.losses.MeanAbsoluteError(\n",
    "    reduction=tf.keras.losses.Reduction.SUM)\n",
    "mae(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 'none' reduction type.  \n",
    "mae = tf.keras.losses.MeanAbsoluteError(\n",
    "    reduction=tf.keras.losses.Reduction.NONE)\n",
    "mae(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5 (Usage with compile() API):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, activation = 'linear', input_shape = (2,)))\n",
    "model.compile(optimizer='sgd', \n",
    "              loss=tf.keras.losses.MeanAbsoluteError())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Mean Squared Logarithmic Error Function\n",
    "\n",
    "The MSLE is define as \n",
    "\n",
    "\\begin{equation}\n",
    "L(y,\\hat{y}) = \\frac{1}{N}\\sum_{i=1}^{N}\\left(log(y_i+1)-log(\\hat{y}_i+1)\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "In Keras, we can use **MeanSquaredLogarithmicError class** as follows:\n",
    "\n",
    "```\n",
    "tf.keras.losses.MeanSquaredLogarithmicError(\n",
    "    reduction=\"auto\", name=\"mean_squared_logarithmic_error\"\n",
    ")\n",
    "```\n",
    "\n",
    "In Keras, we can use **mean_squared_logarithmic_error function** as follows:\n",
    "\n",
    "```\n",
    "tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)\n",
    "```\n",
    "***\n",
    "\n",
    "**Use MSLE when doing regression, believing that your target, conditioned on the input, is normally distributed, and you don’t want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are other metrics as well, you can learn more about them by visiting the references given at the end of this tutorial.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Loss Functions in Keras](https://keras.io/api/losses/)\n",
    "2. [Loss Functions](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions)\n",
    "3. [Sparse Categorical Crossentropy](https://sanjivgautamofficial.medium.com/categorical-cross-entropy-vs-sparse-categorical-cross-entropy-b6a24de2b7f0)\n",
    "4. [Regression Losses](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda0153f39fba084bbcb5c9acd2b000e8a8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
