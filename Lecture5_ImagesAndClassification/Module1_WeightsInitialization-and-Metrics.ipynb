{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights Initialization and Metrics\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "In this module, we will learn:\n",
    "    1. Weights and Biass Initialization\n",
    "    2. Metrics to Evaluate the Preformance of a Model\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Initialization\n",
    "\n",
    "\n",
    "Initializers define the way to set the initial random weights of Keras layers. The keyword arguments used for passing initializers to layers depends on the layer. Usually, it is simply `kernel_initializer` and `bias_initializer`\n",
    "\n",
    "We can use the built-in initialers or define our own. The built-in itializers in the keras library as as follows:\n",
    "1. RandomNormal class\n",
    "2. RandomUniform class\n",
    "3. TruncatedNormal class\n",
    "4. Zeros class\n",
    "5. Ones class\n",
    "6. GlorotNormal class\n",
    "7. GlorotUniform class\n",
    "8. HeNormal class\n",
    "9. HeUniform class\n",
    "10. Identity class\n",
    "11. Orthogonal class\n",
    "12. Constant class\n",
    "13. VarianceScaling class\n",
    "\n",
    "In this module, we will study a few of the built-in initilizers. The details of all the [initializers](https://keras.io/api/layers/initializers/) can be found on keras website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomNormal Class\n",
    "\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `RandomNormal` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : [[ 0.00633252]\n",
      " [-0.02465083]]\n",
      "\n",
      "\n",
      "The bias of the Layer 1 is : [0.]\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=1)\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : {}\\n\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `RandomNormal` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[ 0.0604134  -0.0541209  -0.0304674  -0.05781521]\n",
      " [-0.0275709  -0.04007006 -0.06057356 -0.00229866]]\n",
      "\n",
      "The bias of the Layer 1 is : \n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "The weights of the Layer 2 are : \n",
      "[[-0.01377632]\n",
      " [-0.01184669]\n",
      " [-0.10030229]\n",
      " [-0.02561714]]\n",
      "\n",
      "The bias of the Layer 2 is : \n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "initializer1 = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=18)\n",
    "initializer2 = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=13)\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer1))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer2))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### RandomUniform Class\n",
    "\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `RandomUniform` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[0.8784882 ]\n",
      " [0.45998025]]\n",
      "\n",
      "The bias of the Layer 1 is : [0.]\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.RandomUniform(minval=0.,maxval=1., seed=1)\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `RandomNormal` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[0.26826763 0.36626482 0.42563546 0.5771897 ]\n",
      " [0.62303483 0.5959183  0.47955918 0.74396324]]\n",
      "\n",
      "The bias of the Layer 1 is : \n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "The weights of the Layer 2 are : \n",
      "[[0.9361063 ]\n",
      " [0.6369631 ]\n",
      " [0.11726046]\n",
      " [0.7102027 ]]\n",
      "\n",
      "The bias of the Layer 2 is : \n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "initializer1 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=18)\n",
    "initializer2 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=13)\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer1))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer2))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Zero Class\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `Zeros` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "The bias of the Layer 1 is : [0.]\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.Zeros()\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `Zeros` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "The bias of the Layer 1 is : \n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "The weights of the Layer 2 are : \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "The bias of the Layer 2 is : \n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.Zeros()\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Ones Class\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `Ones` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[1.]\n",
      " [1.]]\n",
      "\n",
      "The bias of the Layer 1 is : [0.]\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.Ones()\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `Ones` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "\n",
      "The bias of the Layer 1 is : \n",
      "[1. 1. 1. 1.]\n",
      "\n",
      "\n",
      "The weights of the Layer 2 are : \n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\n",
      "The bias of the Layer 2 is : \n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "initializer1 = tf.keras.initializers.Ones()\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer,\n",
    "               bias_initializer = initializer))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "***\n",
    "### Zero Class\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `RandomUniform` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.RandomUniform(minval=0.,maxval=1., seed=1)\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `RandomNormal` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer1 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=18)\n",
    "initializer2 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=13)\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer1))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer2))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "***\n",
    "### Zero Class\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `RandomUniform` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.RandomUniform(minval=0.,maxval=1., seed=1)\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `RandomNormal` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer1 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=18)\n",
    "initializer2 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=13)\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer1))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer2))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "***\n",
    "### Zero Class\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `RandomUniform` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.RandomUniform(minval=0.,maxval=1., seed=1)\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `RandomNormal` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer1 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=18)\n",
    "initializer2 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=13)\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer1))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer2))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "***\n",
    "### Zero Class\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `RandomUniform` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.RandomUniform(minval=0.,maxval=1., seed=1)\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `RandomNormal` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer1 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=18)\n",
    "initializer2 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=13)\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer1))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer2))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda0153f39fba084bbcb5c9acd2b000e8a8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
