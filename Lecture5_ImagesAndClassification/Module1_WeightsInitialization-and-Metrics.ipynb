{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights Initialization and Metrics\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "In this module, we will learn:\n",
    "    1. Weights and Biass Initialization\n",
    "    2. Metrics to Evaluate the Preformance of a Model\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Initialization\n",
    "\n",
    "We should not initialize the weights with zeroes or randomly (without knowing the distribution):\n",
    "\n",
    "1. If the weights in a network start too small, then the signal shrinks as it passes through each layer until it’s too tiny to be useful.\n",
    "2. If the weights in a network start too large, then the signal grows as it passes through each layer until it’s too massive to be useful.\n",
    "\n",
    "Initializers define the way to set the initial random weights of Keras layers. The keyword arguments used for passing initializers to layers depends on the layer. Usually, it is simply `kernel_initializer` and `bias_initializer`\n",
    "\n",
    "We can use the built-in initialers or define our own. The built-in itializers in the keras library as as follows:\n",
    "1. RandomNormal class\n",
    "2. RandomUniform class\n",
    "3. TruncatedNormal class\n",
    "4. Zeros class\n",
    "5. Ones class\n",
    "6. GlorotNormal class\n",
    "7. GlorotUniform class\n",
    "8. HeNormal class\n",
    "9. HeUniform class\n",
    "10. Identity class\n",
    "11. Orthogonal class\n",
    "12. Constant class\n",
    "13. VarianceScaling class\n",
    "\n",
    "In this module, we will study a few of the built-in initilizers. The details of all the [initializers](https://keras.io/api/layers/initializers/) can be found on keras website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomNormal Class\n",
    "\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `RandomNormal` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : [[ 0.00633252]\n",
      " [-0.02465083]]\n",
      "\n",
      "\n",
      "The bias of the Layer 1 is : [0.]\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=1)\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : {}\\n\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `RandomNormal` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[ 0.0604134  -0.0541209  -0.0304674  -0.05781521]\n",
      " [-0.0275709  -0.04007006 -0.06057356 -0.00229866]]\n",
      "\n",
      "The bias of the Layer 1 is : \n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "The weights of the Layer 2 are : \n",
      "[[-0.01377632]\n",
      " [-0.01184669]\n",
      " [-0.10030229]\n",
      " [-0.02561714]]\n",
      "\n",
      "The bias of the Layer 2 is : \n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "initializer1 = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=18)\n",
    "initializer2 = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=13)\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer1))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer2))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### RandomUniform Class\n",
    "\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `RandomUniform` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[0.8784882 ]\n",
      " [0.45998025]]\n",
      "\n",
      "The bias of the Layer 1 is : [0.]\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.RandomUniform(minval=0.,maxval=1., seed=1)\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `RandomNormal` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[0.26826763 0.36626482 0.42563546 0.5771897 ]\n",
      " [0.62303483 0.5959183  0.47955918 0.74396324]]\n",
      "\n",
      "The bias of the Layer 1 is : \n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "The weights of the Layer 2 are : \n",
      "[[0.9361063 ]\n",
      " [0.6369631 ]\n",
      " [0.11726046]\n",
      " [0.7102027 ]]\n",
      "\n",
      "The bias of the Layer 2 is : \n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "initializer1 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=18)\n",
    "initializer2 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=13)\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer1))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer2))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Zero Class\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `Zeros` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "The bias of the Layer 1 is : [0.]\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.Zeros()\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `Zeros` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "The bias of the Layer 1 is : \n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "The weights of the Layer 2 are : \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "The bias of the Layer 2 is : \n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.Zeros()\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Ones Class\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `Ones` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[1.]\n",
      " [1.]]\n",
      "\n",
      "The bias of the Layer 1 is : [0.]\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.Ones()\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `Ones` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "\n",
      "The bias of the Layer 1 is : \n",
      "[1. 1. 1. 1.]\n",
      "\n",
      "\n",
      "The weights of the Layer 2 are : \n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\n",
      "The bias of the Layer 2 is : \n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "initializer1 = tf.keras.initializers.Ones()\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer,\n",
    "               bias_initializer = initializer))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### GlorotNormal Class\n",
    "\n",
    "The Glorot normal initializer, also called Xavier normal initializer, can be initialzed using `tf.keras.initializers.glorot_normal`.\n",
    "\n",
    "Xavier Initialization initializes the weights in a network by drawing them from truncated normal distribution centered on 0 with the following standard deviation\n",
    "\n",
    "\\begin{equation}\n",
    "stddev = \\sqrt{\\frac{2}{f_{an\\_in}+f_{an\\_out}}}\n",
    "\\end{equation}\n",
    "\n",
    "where $f_{an\\_in}$ and $f_{an\\_out}$ is the number of input neurons and the number of output neurons, respectively, in weight tensors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `RandomUniform` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[ 0.11756085]\n",
      " [-0.4576337 ]]\n",
      "\n",
      "The bias of the Layer 1 is : [0.]\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.GlorotNormal(seed = 1)\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `GlorotNormal` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[ 0.7930576  -0.7104549  -0.39995104 -0.7589507 ]\n",
      " [ 0.28310135 -0.3424158   0.20694703 -0.23174384]]\n",
      "\n",
      "The bias of the Layer 1 is : \n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "The weights of the Layer 2 are : \n",
      "[[-0.19810493]\n",
      " [-0.17035674]\n",
      " [-0.36837715]\n",
      " [-0.37561056]]\n",
      "\n",
      "The bias of the Layer 2 is : \n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "initializer1 = tf.keras.initializers.GlorotNormal( seed=18)\n",
    "initializer2 = tf.keras.initializers.GlorotNormal(seed = 13)\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer1))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer2))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### GlorotUniform class\n",
    "\n",
    "The Glorot uniform initializer, also called Xavier uniform initializer, is initialized using `tf.keras.initializers.glorot_uniform`.\n",
    "\n",
    "It draws samples from a uniform distribution within $[-limit, limit]$, where \n",
    "\\begin{equation}\n",
    "limit = \\sqrt{\\frac{6}{f_{an\\_in} + f_{an\\_out}}}\n",
    "\\end{equation}\n",
    "\n",
    "where $f_{an\\_in}$ is the number of input units in the weight tensor and $f_{an\\_out}$ is the number of output units).\n",
    "\n",
    "\n",
    "\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `GlorotUniform` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[ 1.0705262 ]\n",
      " [-0.11319292]]\n",
      "\n",
      "The bias of the Layer 1 is : [0.]\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.GlorotUniform(seed=1)\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `GlorotUniform` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the Layer 1 are : \n",
      "[[-0.46346474 -0.26747036 -0.14872909  0.15437937]\n",
      " [ 0.24606967  0.1918366  -0.04088163  0.48792648]]\n",
      "\n",
      "The bias of the Layer 1 is : \n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "The weights of the Layer 2 are : \n",
      "[[ 0.955461  ]\n",
      " [ 0.30007124]\n",
      " [-0.8385403 ]\n",
      " [ 0.460531  ]]\n",
      "\n",
      "The bias of the Layer 2 is : \n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "initializer1 = tf.keras.initializers.GlorotUniform(seed=18)\n",
    "initializer2 = tf.keras.initializers.GlorotUniform(seed=13)\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer1))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer2))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "***\n",
    "### Zero Class\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `RandomUniform` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.RandomUniform(minval=0.,maxval=1., seed=1)\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `RandomNormal` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer1 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=18)\n",
    "initializer2 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=13)\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer1))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer2))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "***\n",
    "### Zero Class\n",
    "Let's define a network with only one layer and one neuron. We will initialize using the `RandomUniform` initializer, as shown in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.RandomUniform(minval=0.,maxval=1., seed=1)\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 1,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer))\n",
    "\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : {}'.format(model.get_weights()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "Let's define a network with two layers: layer 1 has four nodes and layer 2 has one neuron. We will initialize using the `RandomNormal` initializer, as shown in the below example:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer1 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=18)\n",
    "initializer2 = tf.keras.initializers.RandomUniform(minval=0., maxval=1., seed=13)\n",
    "\n",
    "\n",
    "model = Sequential(name = 'Model') \n",
    "model.add(Dense(units = 4,  activation='sigmoid', input_shape=(2,),name='Layer_1',\n",
    "                      kernel_initializer = initializer1))\n",
    "model.add(Dense(units = 1,  activation='sigmoid', name='Layer_2',\n",
    "                      kernel_initializer = initializer2))\n",
    "print('The weights of the Layer 1 are : \\n{}\\n'.format(model.get_weights()[0]))\n",
    "print('The bias of the Layer 1 is : \\n{}\\n\\n'.format(model.get_weights()[1]))\n",
    "print('The weights of the Layer 2 are : \\n{}\\n'.format(model.get_weights()[2]))\n",
    "print('The bias of the Layer 2 is : \\n{}'.format(model.get_weights()[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Xavier and He Normal Initialization](https://prateekvishnu.medium.com/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528)\n",
    "2. [Weights and Biases Initialization](https://keras.io/api/layers/initializers/#glorotnormal-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda0153f39fba084bbcb5c9acd2b000e8a8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
