{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Module1_CallBacks-In-TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAknAZSu9Sbn"
      },
      "source": [
        "# Callbacks in TensorFlow\n",
        "\n",
        "If you are building deep learning models, you may need to sit for hours (or even days) before you can see any real results. You may need to stop model training to change the learning rate, push training logs to the database for future use, or show the training progress in TensorBoard. It seems that we may need to do a lot of work to achieve these basic tasksâ€”that's where TensorFlow callbacks come into the picture.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-KE7AxiiUvY"
      },
      "source": [
        "## What's a Callback Function?\n",
        "\n",
        "Callbacks are the special utilities or functions that are executed during training at given stages of the training procedure. Callbacks can help you prevent overfitting, visualize training progress, debug your code, save checkpoints, generate logs, create a TensorBoard, etc. There are many callbacks readily available in TensorFlow, and you can use multiple. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFxzFt8VjFoy"
      },
      "source": [
        "### When a Callback is Triggered\n",
        "\n",
        "Callbacks are called when a certain event is triggered. There are a few types of events during training that can lead to the trigger of a callback, such as:\n",
        "\n",
        "1. **on_epoch_begin**: as the name suggests, this event is triggered when a new epoch starts.\n",
        "2. **on_epoch_end:** this is triggered when an epoch ends.\n",
        "3. **on_batch_begin:** this is triggered when a new batch is passed for training.\n",
        "4. **on_batch_end:** when a batch is finished with training.\n",
        "5. **on_train_begin:** when the training starts.\n",
        "6. **on_train_end:** when the training ends.\n",
        "\n",
        "To use any callback in the model training you just need to pass the callback object in the `model.fit` call, for example:\n",
        "```\n",
        "model.fit(x, y, callbacks=list_of_callbacks)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTM6xBtHkAXq"
      },
      "source": [
        "## Available Callbacks in TensorFlow\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-RJ9pwylHmD"
      },
      "source": [
        "### 1. Early Stopping:\n",
        "\n",
        "This callback is used very often. This allows us to monitor our metrics, and stop model training when it stops improving. For example, assume that you want to stop training if the accuracy is not improving by $0.05$ or reached at $98\\%$; you can use this callback to do so. This is useful in preventing overfitting of a model, to some extent.\n",
        "```\n",
        "tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "  min_delta=0, \n",
        "  patience=0, \n",
        "  verbose=0, \n",
        "  mode='auto', \n",
        "  baseline=None, \n",
        "  restore_best_weights=False)\n",
        "```\n",
        "\n",
        "**monitor:** the names of the metrics we want to monitor.\n",
        "\n",
        "**min_delta:** the minimum amount of improvement we expect in every epoch.\n",
        "\n",
        "**patience:** the number of epochs to wait before stopping the training.\n",
        "verbose: whether or not to print additional logs.\n",
        "\n",
        "**mode:** defines whether the monitored metrics should be increasing, decreasing, or inferred from the name; possible values are 'min', 'max', or 'auto'.\n",
        "baseline: values for the monitored metrics.\n",
        "\n",
        "**restore_best_weights:** if set to True, the model will get the weights of the epoch which has the best value for the monitored metrics; otherwise, it will get the weights of the last epoch.\n",
        "\n",
        "**The EarlyStopping callback is executed via the on_epoch_end trigger for training.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0du42Kgl7r3"
      },
      "source": [
        "### 2. ModelCheckpoint\n",
        "\n",
        "This callback allows us to save the model regularly during training. This is especially useful when training deep learning models which take a long time to train. This callback monitors the training and saves model checkpoints at regular intervals, based on the metrics.\n",
        "\n",
        "```\n",
        "tf.keras.callbacks.ModelCheckpoint(filepath, \n",
        "      monitor='val_loss', \n",
        "      verbose=0, \n",
        "      save_best_only=False,\n",
        "      save_weights_only=False, \n",
        "      mode='auto', \n",
        "      save_freq='epoch')\n",
        "```\n",
        "\n",
        "**filepath:** path for saving the model. You can pass the file path with formatting options like model-{epoch:02d}-{val_loss:0.2f}; this saves the model with the mentioned values in the name.\n",
        "\n",
        "**monitor:** name of the metrics to monitor.\n",
        "\n",
        "**save_best_only:** if True, the best model will not be overridden.\n",
        "\n",
        "**mode:** defines whether the monitored metrics should be increasing, decreasing, or inferred from the name; possible values are 'min', 'max', or 'auto'.\n",
        "\n",
        "**save_weights_only:** if True, only the weights of the models will be saved. Otherwise the full model will be saved.\n",
        "\n",
        "**save_freq:** if 'epoch', the model will be saved after every epoch. If an integer value is passed, the model will be saved after the integer number of batches (not to be confused with epochs).\n",
        "\n",
        "The `ModelCheckpoint` callback is executed via the on_epoch_end trigger of training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qKXps81mkql"
      },
      "source": [
        "### 3. TensorBoard\n",
        "\n",
        "This is one of the best callbacks if you want to visualize the training summary for your model. This callback generates the logs for TensorBoard, which you can later launch to visualize the progress of your training.\n",
        "\n",
        "```\n",
        "tf.keras.callbacks.TensorBoard(log_dir='logs',\n",
        "                                 histogram_freq=0, \n",
        "                                 write_graph=True, \n",
        "                                 write_images=False,    \n",
        "                                 update_freq='epoch', \n",
        "                                 profile_batch=2, \n",
        "                                 embeddings_freq=0,    \n",
        "                                 embeddings_metadata=None, \n",
        "                                 **kwargs)\n",
        "```\n",
        "\n",
        "The `TensorBoard` callback is also triggered at `on_epoch_end`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLrR8oB4m-30"
      },
      "source": [
        "### 4. LearningRateScheduler\n",
        "\n",
        "This callback is handy in scenarios where the user wants to update the learning rate as training progresses. For instance, as the training progresses you may want to decrease the learning rate after a certain number of epochs. The `LearningRateScheduler` will let you do exactly that.\n",
        "\n",
        "```\n",
        "tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0)\n",
        "```\n",
        "\n",
        "**schedule:** this is a function that takes the epoch index and returns a new learning rate.\n",
        "\n",
        "**verbose:** whether or not to print additional logs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0oCN6J4oXhR"
      },
      "source": [
        "### 5. CSVLogger\n",
        "\n",
        "As the name suggests, this callback logs the training details in a CSV file. The logged parameters are epoch, accuracy, loss, val_accuracy, and val_loss. One thing to keep in mind is that you need to pass accuracy as a metric while compiling the model, otherwise you will get an execution error.\n",
        "```\n",
        "tf.keras.callbacks.CSVLogger(filename, \n",
        "                             separator=',', \n",
        "                             append=False)\n",
        "```\n",
        "\n",
        "The logger accepts the filename, separator, and append as parameters. append defines whether or not to append to an existing file, or write in a new file instead.\n",
        "\n",
        "The CSVLogger callback is executed via the `on_epoch_end` trigger of training. So when an epoch ends, the logs are put into a file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCC_reQ1ojgB"
      },
      "source": [
        "### 6. LambdaCallback\n",
        "\n",
        "This callback is required when you need to call some custom function on any of the events, and the provided callbacks do not suffice. For instance, say you want to put your logs into a database.\n",
        "\n",
        "```\n",
        "tf.keras.callbacks.LambdaCallback(on_epoch_begin=None, \n",
        "                                  on_epoch_end=None, \n",
        "                                  on_batch_begin=None, \n",
        "                                  on_batch_end=None,    \n",
        "                                  on_train_begin=None, \n",
        "                                  on_train_end=None, \n",
        "                                  **kwargs)\n",
        "```\n",
        "\n",
        "All the parameters of this callback expect a function which takes the arguments specified here:\n",
        "\n",
        "- on_epoch_begin and on_epoch_end: epoch, logs\n",
        "- on_batch_begin and on_batch_end: batch, logs\n",
        "- on_train_begin and on_train_end: logs\n",
        "\n",
        "\n",
        "This callback is called for all the events, and executes the custom functions based on the parameters passed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFKLDML9o80d"
      },
      "source": [
        "### 7. ReduceLROnPlateau\n",
        "\n",
        "This callback is used when you want to change the learning rate when the metrics have stopped improving. As opposed to `LearningRateScheduler`, it will reduce the learning based on the metric (not epoch).\n",
        "\n",
        "```\n",
        "tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
        "                                     factor=0.1, \n",
        "                                     patience=10, \n",
        "                                     verbose=0, \n",
        "                                     mode='auto',    \n",
        "                                     min_delta=0.0001, \n",
        "                                     cooldown=0, \n",
        "                                     min_lr=0, \n",
        "                                     **kwargs)\n",
        "```\n",
        "\n",
        "Many of the parameters are similar to the `EarlyStoppingCallback`, so let's focus on those that are different.\n",
        "\n",
        "monitor, patience, verbose, mode, min_delta: these are similar to `EarlyStopping`.\n",
        "\n",
        "**factor:** the factor by which the learning rate should be decreased (new learning rate = old learning rate * factor).\n",
        "\n",
        "**cooldown:** the number of epochs to wait before restarting the monitoring of the metrics.\n",
        "\n",
        "**min_lr:** the minimum bound for the learning rate (the learning rate canâ€™t go below this).\n",
        "\n",
        "This callback is also called at the `on_epoch_end` event."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAlUi-MnpSiU"
      },
      "source": [
        "### 8. RemoteMonitor\n",
        "\n",
        "This callback is useful when you want to post the logs to an API. This callback can also be mimicked using LambdaCallback.\n",
        "```\n",
        "tf.keras.callbacks.RemoteMonitor(root='http://localhost:9000',                \n",
        "                                   path='/publish/epoch/end/', \n",
        "                                   field='data',\n",
        "                                   headers=None, \n",
        "                                   send_as_json=False)\n",
        "```\n",
        "\n",
        "**root:** this is the URL.\n",
        "\n",
        "**path:** this is the endpoint name/path.\n",
        "\n",
        "**field:** this is the name of the key which will have all the logs.\n",
        "\n",
        "**header:** the header which needs to be sent.\n",
        "\n",
        "**send_as_json:** if True, the data will be sent in JSON format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EXaQbtzprYA"
      },
      "source": [
        "### 9. BaseLogger & History\n",
        "\n",
        "These two callbacks are automatically applied to all Keras models. The history object is returned by `model.fit`, and contains a dictionary with the average accuracy and loss over the epochs. The parameters property contains the dictionary with the parameters used for training (epochs, steps, verbose). If you have a callback for changing the learning rate, then that will also be part of the history object.\n",
        "\n",
        "\n",
        "BaseLogger accumulates an average of your metrics across epochs. So the metrics you see at the end of the epoch are an average of all the metrics over all the batches.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ6eo7bCp3UB"
      },
      "source": [
        "### 10. TerminateOnNaN\n",
        "\n",
        "This callback terminates the training if the loss becomes NaN.\n",
        "```\n",
        "tf.keras.callbacks.TerminateOnNaN()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tdykzTc9uaB"
      },
      "source": [
        "# References\n",
        "\n",
        "1. [A Guide to TensorFlow Callbacks](https://blog.paperspace.com/tensorflow-callbacks/)\n",
        "2. [Writing your own callbacks](https://www.tensorflow.org/guide/keras/custom_callback)\n",
        "3. [tf.keras.callbacks.Callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback)\n",
        "4. [A Practical Introduction to Keras Callbacks in TensorFlow 2](https://towardsdatascience.com/a-practical-introduction-to-keras-callbacks-in-tensorflow-2-705d0c584966)\n",
        "5. [How to use TensorFlow callbacks?](https://medium.com/ydata-ai/how-to-use-tensorflow-callbacks-f54f9bb6db25)\n",
        "6. [Callbacks in Tensor Flow](https://sailajakarra.medium.com/callbacks-in-tensor-flow-f8e7f9996f5f)\n",
        "7. [Keras Callbacks Explained In Three Minutes](https://www.kdnuggets.com/2019/08/keras-callbacks-explained-three-minutes.html)\n"
      ]
    }
  ]
}