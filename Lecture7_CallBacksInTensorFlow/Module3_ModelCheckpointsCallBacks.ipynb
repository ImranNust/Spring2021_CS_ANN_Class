{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module3_ModelCheckpointsCallBacks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9H9iU_Xfxw7"
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-RJ9pwylHmD"
      },
      "source": [
        "# ModelCheckpoint\n",
        "\n",
        "`ModelCheckpoint` is a callback to save the Keras model or model weight during training, so the model or weights can be loaded later to continue the training from the state saved.\n",
        "\n",
        "This callback allows us to save the model regularly during training. This is especially useful when training deep learning models which take a long time to train. This callback monitors the training and saves model checkpoints at regular intervals, based on the metrics.\n",
        "\n",
        "```\n",
        "tf.keras.callbacks.ModelCheckpoint(filepath, \n",
        "      monitor='val_loss', \n",
        "      verbose=0, \n",
        "      save_best_only=False,\n",
        "      save_weights_only=False, \n",
        "      mode='auto', \n",
        "      save_freq='epoch')\n",
        "```\n",
        "\n",
        "**filepath:** path for saving the model. You can pass the file path with formatting options like model-{epoch:02d}-{val_loss:0.2f}; this saves the model with the mentioned values in the name.\n",
        "\n",
        "**monitor:** name of the metrics to monitor.\n",
        "\n",
        "**save_best_only:** if True, the best model will not be overridden.\n",
        "\n",
        "**mode:** defines whether the monitored metrics should be increasing, decreasing, or inferred from the name; possible values are 'min', 'max', or 'auto'.\n",
        "\n",
        "**save_weights_only:** if True, only the weights of the models will be saved. Otherwise the full model will be saved.\n",
        "\n",
        "**save_freq:** if 'epoch', the model will be saved after every epoch. If an integer value is passed, the model will be saved after the integer number of batches (not to be confused with epochs).\n",
        "\n",
        "The `ModelCheckpoint` callback is executed via the on_epoch_end trigger of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xED0LqgxW_NH"
      },
      "source": [
        "***\n",
        "## Preparing Dataset \n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6HxSnMJf56U",
        "outputId": "570ebcc4-0ca9-4971-958e-6bed0e6c0f05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_labels = train_labels[:1000]\n",
        "test_labels = test_labels[:1000]\n",
        "\n",
        "train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
        "test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10SG31C8soCw"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3XG0HNUgFax",
        "outputId": "acab8980-5bee-4179-f547-ef7c9449495e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define a simple sequential model\n",
        "def create_model():\n",
        "  model = Sequential([\n",
        "    Dense(512, activation='relu', input_shape=(784,)),\n",
        "    Dropout(0.2),\n",
        "    Dense(10)\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "  return model\n",
        "\n",
        "# Create a basic model instance\n",
        "model = create_model()\n",
        "\n",
        "# Display the model's architecture\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80M89qQ3ssDu"
      },
      "source": [
        "## Saving only Weights\n",
        "\n",
        "Using `ModelCheckpoint`, we have the option either to save the entire model or to save weights only. In the example below, we will save only weight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrotL1YxticU"
      },
      "source": [
        "\n",
        "\n",
        "Let's instantitate the `ModelCheckPoint` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSkzsoHQuAwg"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_path = '/content/model_checkpoints/'\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    save_freq = 'epoch',\n",
        "    save_weights_only = True,\n",
        "    verbose = 1\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-ADEUmVsubW"
      },
      "source": [
        "\n",
        "\n",
        "Let's train for $5$ epochs and save weights only. Later we will upload the weights and check the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyguG9FpxD8u",
        "outputId": "bfb61d28-e865-481d-c70a-1cd987f16340"
      },
      "source": [
        "# Train the model with the new callback\n",
        "model.fit(train_images, \n",
        "          train_labels,  \n",
        "          epochs=10,\n",
        "          validation_data=(test_images, test_labels),\n",
        "          callbacks=[checkpoint])  # Pass callback to training"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.1513 - sparse_categorical_accuracy: 0.6840 - val_loss: 0.6846 - val_sparse_categorical_accuracy: 0.7980\n",
            "\n",
            "Epoch 00001: saving model to /content/model_checkpoints/\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4093 - sparse_categorical_accuracy: 0.8950 - val_loss: 0.5128 - val_sparse_categorical_accuracy: 0.8380\n",
            "\n",
            "Epoch 00002: saving model to /content/model_checkpoints/\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.2839 - sparse_categorical_accuracy: 0.9220 - val_loss: 0.4458 - val_sparse_categorical_accuracy: 0.8590\n",
            "\n",
            "Epoch 00003: saving model to /content/model_checkpoints/\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2015 - sparse_categorical_accuracy: 0.9580 - val_loss: 0.4073 - val_sparse_categorical_accuracy: 0.8720\n",
            "\n",
            "Epoch 00004: saving model to /content/model_checkpoints/\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.1516 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.4073 - val_sparse_categorical_accuracy: 0.8660\n",
            "\n",
            "Epoch 00005: saving model to /content/model_checkpoints/\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.1153 - sparse_categorical_accuracy: 0.9740 - val_loss: 0.4071 - val_sparse_categorical_accuracy: 0.8690\n",
            "\n",
            "Epoch 00006: saving model to /content/model_checkpoints/\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0850 - sparse_categorical_accuracy: 0.9850 - val_loss: 0.4153 - val_sparse_categorical_accuracy: 0.8590\n",
            "\n",
            "Epoch 00007: saving model to /content/model_checkpoints/\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0665 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.3937 - val_sparse_categorical_accuracy: 0.8690\n",
            "\n",
            "Epoch 00008: saving model to /content/model_checkpoints/\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0458 - sparse_categorical_accuracy: 0.9980 - val_loss: 0.4072 - val_sparse_categorical_accuracy: 0.8750\n",
            "\n",
            "Epoch 00009: saving model to /content/model_checkpoints/\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0347 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3848 - val_sparse_categorical_accuracy: 0.8780\n",
            "\n",
            "Epoch 00010: saving model to /content/model_checkpoints/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7e6eb1ead0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkgRroPWkZQq"
      },
      "source": [
        "Let's check test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0uyuZ2FkdLa",
        "outputId": "3bcaa653-96e7-424e-beee-d4e21c4176f6"
      },
      "source": [
        "model.evaluate(x = test_images, y = test_labels)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3848 - sparse_categorical_accuracy: 0.8780\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3847571611404419, 0.878000020980835]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNi9g4DqhI_z"
      },
      "source": [
        "As long as two models share the same architecture you can share weights between them. So, when restoring a model from weights-only, create a model with the same architecture as the original model and then set its weights.\n",
        "\n",
        "Now rebuild a fresh, then load the weights from the checkpoint and re-evaluate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af8ulVlyhVkd",
        "outputId": "7707d84a-90bf-4711-c9b8-32df9c1b209b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "new_model = create_model()\n",
        "new_model.load_weights(checkpoint_path)\n",
        "new_model.evaluate(x = test_images, y=test_labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 2ms/step - loss: 0.3848 - sparse_categorical_accuracy: 0.8780\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3847571611404419, 0.878000020980835]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTDBDxRkhywf"
      },
      "source": [
        "It can be seen that both models have predicted same test accuracy.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRF1OLVCj8Px"
      },
      "source": [
        "## Checkpoint callback options\n",
        "The callback provides several options to provide unique names for checkpoints and adjust the checkpointing frequency.\n",
        "\n",
        "Train a new model, and save uniquely named checkpoints once every five epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3uxe_eKkHC0",
        "outputId": "158f7424-a986-4709-8f6c-4d8bf665a587",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Include the epoch in the file name (uses `str.format`)\n",
        "checkpoint_path = '/content/model_checkpoints/cp-{epoch:04d}.ckpt'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "batch_size = 32\n",
        "# Create a callback that saves the model's weights every 5 epochs\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True,\n",
        "    save_freq = 5*batch_size\n",
        ")\n",
        "\n",
        "# Create a new model instance\n",
        "model = create_model()\n",
        "\n",
        "# Save the weights using the `checkpoint_path` format\n",
        "model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "# Train the model with the new callback\n",
        "model.fit(train_images, \n",
        "          train_labels,\n",
        "          epochs=50, \n",
        "          batch_size=batch_size, \n",
        "          callbacks=[checkpoint],\n",
        "          validation_data=(test_images, test_labels),\n",
        "          verbose=0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00005: saving model to /content/model_checkpoints/cp-0005.ckpt\n",
            "\n",
            "Epoch 00010: saving model to /content/model_checkpoints/cp-0010.ckpt\n",
            "\n",
            "Epoch 00015: saving model to /content/model_checkpoints/cp-0015.ckpt\n",
            "\n",
            "Epoch 00020: saving model to /content/model_checkpoints/cp-0020.ckpt\n",
            "\n",
            "Epoch 00025: saving model to /content/model_checkpoints/cp-0025.ckpt\n",
            "\n",
            "Epoch 00030: saving model to /content/model_checkpoints/cp-0030.ckpt\n",
            "\n",
            "Epoch 00035: saving model to /content/model_checkpoints/cp-0035.ckpt\n",
            "\n",
            "Epoch 00040: saving model to /content/model_checkpoints/cp-0040.ckpt\n",
            "\n",
            "Epoch 00045: saving model to /content/model_checkpoints/cp-0045.ckpt\n",
            "\n",
            "Epoch 00050: saving model to /content/model_checkpoints/cp-0050.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7e60317850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd_UxtcRlXWR"
      },
      "source": [
        "To test, reset the model and load the latest checkpoint:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsP8K8WhlrYP",
        "outputId": "fd6ca584-06cf-4a43-c019-ceb339570af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Since there are many checkpoints, but we will load the latest checkpoints only\n",
        "\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/model_checkpoints/cp-0050.ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vLSkiH0lXzM",
        "outputId": "a1702f73-533b-4205-93c1-c765cf85239d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create a new model instance\n",
        "model = create_model()\n",
        "\n",
        "# Load the previously saved weights\n",
        "model.load_weights(latest)\n",
        "\n",
        "# Re-evaluate the model\n",
        "loss, acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 - 0s - loss: 0.4689 - sparse_categorical_accuracy: 0.8780\n",
            "Restored model, accuracy: 87.80%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wraVVp4EoY67"
      },
      "source": [
        "## Saving the Entire Model\n",
        "\n",
        "Call `model.save` to save a model's architecture, weights, and training configuration in a single file/folder. This allows you to export a model so it can be used without access to the original Python code. Since the optimizer-state is recovered, you can resume training from exactly where you left off.\n",
        "\n",
        "An entire model can be saved in two different file formats (SavedModel and HDF5). The TensorFlow SavedModel format is the default file format in TF2.x. However, models can be saved in HDF5 format. \n",
        "\n",
        "Saving a fully-functional model is very useful—you can load them in TensorFlow.js (Saved Model, HDF5) and then train and run them in web browsers, or convert them to run on mobile devices using TensorFlow Lite (Saved Model, HDF5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEyxHspgsHn9"
      },
      "source": [
        "Before proceeding, let's delete all the folders containing any weights or models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVnryoOdsNWw",
        "outputId": "cff93554-ff87-4dfc-e98b-e7beb46542d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm -r save_model/\n",
        "!rm -r model_checkpoints"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'save_model/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "var14r3poxr1",
        "outputId": "a642eaa6-81f1-4515-a33d-1111e361c962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Let's create a fresh model\n",
        "model1 = create_model()\n",
        "\n",
        "# Defining the path\n",
        "save_model_path = '/content/save_model/'\n",
        "\n",
        "# Instantiating the callback to save the entire model\n",
        "save_model_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = save_model_path,\n",
        "    save_best_only = False,\n",
        "    save_weights_only = False\n",
        "    )\n",
        "\n",
        "# training the model \n",
        "model1.fit(x = train_images, y = train_labels, epochs = 5, validation_data = (test_images, test_labels),\n",
        "           callbacks = [save_model_callback])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1.1527 - sparse_categorical_accuracy: 0.6820 - val_loss: 0.7194 - val_sparse_categorical_accuracy: 0.7740\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.4196 - sparse_categorical_accuracy: 0.8810 - val_loss: 0.5302 - val_sparse_categorical_accuracy: 0.8320\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2952 - sparse_categorical_accuracy: 0.9240 - val_loss: 0.4741 - val_sparse_categorical_accuracy: 0.8530\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.2118 - sparse_categorical_accuracy: 0.9460 - val_loss: 0.4477 - val_sparse_categorical_accuracy: 0.8620\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.1700 - sparse_categorical_accuracy: 0.9620 - val_loss: 0.4283 - val_sparse_categorical_accuracy: 0.8580\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7e5e95ffd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGZO8cUOsS8_"
      },
      "source": [
        "Let's compute the test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4vnBi8Vrtyk",
        "outputId": "c716bdaf-c44d-40b3-94a0-21294c3e7209",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model1.evaluate(x = test_images, y = test_labels)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4283 - sparse_categorical_accuracy: 0.8580\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.42828598618507385, 0.8579999804496765]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWXgkTzcscDH"
      },
      "source": [
        "Let's load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kicxCE8gwsw7",
        "outputId": "317dff3e-7e80-4bd3-ee43-0ae85a280b42"
      },
      "source": [
        "new_model = tf.keras.models.load_model(filepath=save_model_path)\n",
        "new_model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OndW0QC0sqSy"
      },
      "source": [
        "You can see that our loaded model is exactly same as it was previously.\n",
        "\n",
        "Let's check the test accuracy on the loaded model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYkjEs1fszdT",
        "outputId": "ad5377ed-c750-4559-e154-16bb7df7ee37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "new_model.evaluate(x=test_images, y = test_labels)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4283 - sparse_categorical_accuracy: 0.8580\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.42828598618507385, 0.8579999804496765]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hY59NLos8yl"
      },
      "source": [
        "We can see that the loaded model's accuracy is same as it was before. Let's continue training it for 15 more epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJC9aCO5w73V",
        "outputId": "44dac62e-96b0-4f9e-ca24-d6c85219b73b"
      },
      "source": [
        "new_history = new_model.fit(x  = train_images, y = train_labels,\n",
        "                            validation_data = (test_images, test_labels),\n",
        "                            epochs = 15,\n",
        "                            callbacks = [save_model_callback])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "32/32 [==============================] - 1s 7ms/step - loss: 0.1630 - sparse_categorical_accuracy: 0.9580 - val_loss: 0.4243 - val_sparse_categorical_accuracy: 0.8530\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 2/15\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0854 - sparse_categorical_accuracy: 0.9820 - val_loss: 0.4196 - val_sparse_categorical_accuracy: 0.8530\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 3/15\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0560 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.4238 - val_sparse_categorical_accuracy: 0.8640\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 4/15\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0388 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.4088 - val_sparse_categorical_accuracy: 0.8620\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 5/15\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.0250 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3950 - val_sparse_categorical_accuracy: 0.8740\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 6/15\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0183 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4105 - val_sparse_categorical_accuracy: 0.8740\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 7/15\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0152 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4210 - val_sparse_categorical_accuracy: 0.8690\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 8/15\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0128 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4344 - val_sparse_categorical_accuracy: 0.8720\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 9/15\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0101 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4512 - val_sparse_categorical_accuracy: 0.8630\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 10/15\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0095 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4436 - val_sparse_categorical_accuracy: 0.8710\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 11/15\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0073 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4439 - val_sparse_categorical_accuracy: 0.8720\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 12/15\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0062 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4576 - val_sparse_categorical_accuracy: 0.8670\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 13/15\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0065 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4608 - val_sparse_categorical_accuracy: 0.8670\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 14/15\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0049 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4507 - val_sparse_categorical_accuracy: 0.8750\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n",
            "Epoch 15/15\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0041 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4585 - val_sparse_categorical_accuracy: 0.8740\n",
            "INFO:tensorflow:Assets written to: /content/save_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "oXeMUL67xLIS",
        "outputId": "5498b313-4251-421f-f0db-dbd67780b35c"
      },
      "source": [
        "plt.figure(figsize = (14, 8))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(new_history.history['loss'], label='loss')\n",
        "plt.plot(new_history.history['val_loss'], label='val_loss')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tdykzTc9uaB"
      },
      "source": [
        "# References\n",
        "\n",
        "1. [A Guide to TensorFlow Callbacks](https://blog.paperspace.com/tensorflow-callbacks/)\n",
        "2. [Writing your own callbacks](https://www.tensorflow.org/guide/keras/custom_callback)\n",
        "3. [tf.keras.callbacks.Callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback)\n",
        "4. [A Practical Introduction to Keras Callbacks in TensorFlow 2](https://towardsdatascience.com/a-practical-introduction-to-keras-callbacks-in-tensorflow-2-705d0c584966)\n",
        "5. [How to use TensorFlow callbacks?](https://medium.com/ydata-ai/how-to-use-tensorflow-callbacks-f54f9bb6db25)\n",
        "6. [Callbacks in Tensor Flow](https://sailajakarra.medium.com/callbacks-in-tensor-flow-f8e7f9996f5f)\n",
        "7. [Keras Callbacks Explained In Three Minutes](https://www.kdnuggets.com/2019/08/keras-callbacks-explained-three-minutes.html)\n"
      ]
    }
  ]
}